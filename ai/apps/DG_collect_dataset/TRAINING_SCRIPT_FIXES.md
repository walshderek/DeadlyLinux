# DG_collect_dataset Training Script Fixes

## Problem Summary
Training scripts generated by `dg_collect_dataset` were failing to run in musubi-tuner due to multiple critical issues preventing cache file generation and proper script execution.

## Root Causes Identified

### 1. **CRITICAL: Missing Cache Flags**
- Generated scripts lacked `--cache_latents` and `--cache_latents_to_disk` flags
- Without these, the training process couldn't pre-generate cache files
- Cache files are essential for efficient training with large models

### 2. **CRITICAL: Wrong Musubi-Tuner Path**
- `utils.py` had incorrect path: `/home/seanf/ai/apps/musubi-tuner`
- Actual location: `/home/seanf/deadlygraphics/ai/apps/musubi-tuner`
- This prevented dataset deployment to the correct location

### 3. **CRITICAL: No Windows TOML Generated**
- Only Linux TOML was being created
- Windows training scripts referenced non-existent `michael_gove_win.toml`
- Training couldn't start without proper configuration file

### 4. **Missing Template Tokens**
- Template had `@DIT_HIGH@` and `@NETWORK_DIM@` tokens
- Replacements dictionary didn't include these tokens
- Resulted in malformed training scripts

### 5. **Venv Activation Issues**
- Original template had venv activation commented out
- No fallback warning if venv wasn't found
- Could lead to dependency conflicts

### 6. **Path Format Inconsistencies**
- Mixed forward slashes and backslashes in Windows paths
- TOML files require forward slashes even on Windows
- Caused path resolution failures

## Fixes Applied

### Fix 1: Corrected Musubi-Tuner Path
**File:** `ai/apps/DG_collect_dataset/core/utils.py`

```python
MUSUBI_PATHS = {
    'wsl_app': "/home/seanf/deadlygraphics/ai/apps/musubi-tuner",  # FIXED
    'wsl_models': "/home/seanf/ai/models",
    'win_app': r"C:\AI\apps\musubi-tuner",
    'win_models': r"C:\AI\models",
    'win_datasets': r"C:\AI\apps\musubi-tuner\files\datasets"
}
```

### Fix 2: Added Windows TOML Generation
**File:** `ai/apps/DG_collect_dataset/core/06_publish.py`

```python
# Generate both TOML files
toml_win = generate_toml(win_image_path, win_cache_path, 256)
toml_linux = generate_toml(linux_image_path, linux_cache_path, 256)

# Write both files
with open(musubi_toml_dir / f"{slug}_win.toml", "w", encoding='utf-8') as f:
    f.write(toml_win)

with open(musubi_toml_dir / f"{slug}_linux.toml", "w", encoding='utf-8') as f:
    f.write(toml_linux)
```

### Fix 3: Added Critical Cache Flags
**File:** `ai/apps/DG_collect_dataset/core/templates/train_template.bat`

```batch
--vae "%VAE%" ^
--cache_latents ^              <-- ADDED
--cache_latents_to_disk ^      <-- ADDED
--vae_cache_cpu ^
--vae_dtype bfloat16 ^
--sdpa
```

### Fix 4: Improved Venv Activation
**File:** `ai/apps/DG_collect_dataset/core/templates/train_template.bat`

```batch
if exist "venv\Scripts\activate.bat" (
    call "venv\Scripts\activate.bat"
) else (
    echo WARNING: venv not found, using system Python
)
```

### Fix 5: Completed Template Replacements
**File:** `ai/apps/DG_collect_dataset/core/06_publish.py`

```python
replacements = {
    "@WAN@": utils.MUSUBI_PATHS['win_app'],
    "@CFG@": f"{utils.MUSUBI_PATHS['win_app']}/files/tomls/{slug}_win.toml".replace("/", "\\"),
    "@OUT@": f"{utils.MUSUBI_PATHS['win_app']}/outputs/{slug}".replace("/", "\\"),
    "@OUTNAME@": slug,
    "@LOGDIR@": f"{utils.MUSUBI_PATHS['win_app']}/logs".replace("/", "\\"),
    "@DIT_LOW@": "C:/AI/models/diffusion_models/Wan/Wan2.2/14B/Wan_2_2_T2V/bf16/Wan-2.2-T2V-Low-Noise-BF16.safetensors",
    "@DIT_HIGH@": "C:/AI/models/diffusion_models/Wan/Wan2.2/14B/Wan_2_2_T2V/bf16/Wan-2.2-T2V-High-Noise-BF16.safetensors",  # ADDED
    "@VAE@": "C:/AI/models/vae/WAN/Wan2.1_VAE.pth",
    "@T5@": "C:/AI/models/clip/models_t5_umt5-xxl-enc-bf16.pth",
    "@GRAD_ACCUM@": "1",
    "@LEARNING_RATE@": "0.0001",
    "@EPOCHS@": "35",
    "@NETWORK_ALPHA@": "16",
    "@NETWORK_DIM@": "16",  # ADDED
    "@N_WORKERS@": "4"
}
```

## Verification

### Generated Files Check
After running `python DG_collect_dataset.py "michael gove" --only-step 6`:

✅ **Dataset Deployed:**
- `/home/seanf/deadlygraphics/ai/apps/musubi-tuner/files/datasets/michael_gove/256/` (97 images)
- Each image has matching `.txt` caption file

✅ **Cache Directories Created:**
- `/home/seanf/deadlygraphics/ai/apps/musubi-tuner/files/datasets/michael_gove/256_cache/`
- `/home/seanf/deadlygraphics/ai/apps/musubi-tuner/files/datasets/michael_gove/512_cache/`
- `/home/seanf/deadlygraphics/ai/apps/musubi-tuner/files/datasets/michael_gove/1024_cache/`

✅ **TOML Files Generated:**
- `michael_gove_win.toml` with Windows paths (forward slashes)
- `michael_gove_linux.toml` with Linux paths

✅ **Training Script Generated:**
- `train_michael_gove.bat` with all required flags
- Includes `--cache_latents` and `--cache_latents_to_disk`
- Proper venv activation
- All paths correctly formatted

### Key Training Script Features
```batch
python -m accelerate.commands.launch --num_processes 1 "wan_train_network.py" ^
    --dataset_config "%CFG%" ^
    --cache_latents ^
    --cache_latents_to_disk ^
    --vae_cache_cpu ^
    --vae_dtype bfloat16 ^
    [... other flags ...]
```

## Path Handling Strategy

### For TOML Files (Both Platforms)
- Always use forward slashes: `C:/AI/apps/musubi-tuner/files/datasets/slug/256`
- Works correctly on both Windows and Linux
- TOML parsers handle forward slashes universally

### For Batch Scripts (Windows)
- Use backslashes for native Windows paths: `C:\AI\apps\musubi-tuner\outputs\slug`
- Except for model paths which use forward slashes

### For Deployment
- Linux source: `/home/seanf/deadlygraphics/ai/apps/DG_collect_dataset/outputs/slug/`
- Deploy to: `/home/seanf/deadlygraphics/ai/apps/musubi-tuner/files/datasets/slug/`
- Creates separate cache directories per resolution

## Usage

### To Regenerate Training Scripts for Any Dataset:
```bash
cd /home/seanf/deadlygraphics/ai/apps/DG_collect_dataset
python DG_collect_dataset.py "Person Name" --only-step 6
```

This will:
1. Deploy images from `outputs/person_name/05_caption/` to musubi-tuner
2. Create separate cache directories (256_cache, 512_cache, 1024_cache)
3. Generate both Windows and Linux TOML files
4. Generate training script with all required cache flags
5. Place script in musubi-tuner root directory

### To Run Training (Windows):
```batch
cd C:\AI\apps\musubi-tuner
train_michael_gove.bat
```

### To Run Training (Linux/WSL):
```bash
cd /home/seanf/deadlygraphics/ai/apps/musubi-tuner
# Activate musubi-tuner's venv first
source venv/bin/activate
bash files/train_michael_gove_256.sh
```

## Critical Success Factors

1. **Cache flags must be present** - Without `--cache_latents` and `--cache_latents_to_disk`, training will fail or be extremely slow
2. **Separate cache directories** - Cache must be in different directory from images
3. **Matching captions** - Every `.jpg` must have corresponding `.txt` file
4. **Correct paths** - TOML paths must match actual dataset location
5. **Venv activation** - Training script must activate musubi-tuner's venv, not DG_collect_dataset's venv

## Testing Checklist

Before running training, verify:
- [ ] Dataset images exist in `musubi-tuner/files/datasets/slug/256/`
- [ ] Each image has matching `.txt` caption file
- [ ] Cache directory exists and is empty: `256_cache/`
- [ ] Windows TOML exists: `files/tomls/slug_win.toml`
- [ ] Training script exists: `train_slug.bat`
- [ ] Script contains `--cache_latents` flag
- [ ] Script contains `--cache_latents_to_disk` flag
- [ ] All model paths in script are valid
- [ ] Musubi-tuner venv exists and has required dependencies

## Next Steps

To train michael_gove model:
1. Navigate to musubi-tuner directory
2. Ensure venv is activated
3. Run: `train_michael_gove.bat` (Windows) or `bash files/train_michael_gove_256.sh` (Linux)
4. Monitor cache generation in first epoch (creates `.npz` files in cache directory)
5. Training should proceed normally after cache files are generated

## Fix 6: Attention Mechanism Flag Requirements
**Date:** 2025-12-25
**Files Affected:** Training batch scripts

### Problem
Training scripts included `--sdpa` argument which was not recognized by caching scripts but IS REQUIRED by the training script:

**Caching Scripts (DON'T support attention flags):**
- `wan_cache_latents.py` - error: unrecognized arguments: --sdpa
- `wan_cache_text_encoder_outputs.py` - error: unrecognized arguments: --sdpa

**Training Script (REQUIRES attention flag):**
- `wan_train_network.py` - ValueError: either --sdpa, --flash-attn, --flash3, --sage-attn or --xformers must be specified

This caused the training pipeline to fail at the first step (VAE latent caching) because --sdpa was being passed to scripts that don't support it.

### Solution
Remove attention flags from caching steps but ADD them to the training step. Use `--sage-attn` (sage attention) since it was successfully imported.

**Changed in `train_theresa_may_full.bat`:**
- Removed `--sdpa` from VAE caching command (Step 1)
- Removed `--sdpa` from text encoder caching command (Step 2)
- Changed `--sdpa` to `--sage-attn` in training command (Step 3)

### Correct Pattern
```batch
:: STEP 1: VAE Caching (NO attention flag)
"%PYTHON_EXE%" "wan_cache_latents.py" ^
    --vae "%VAE_PATH%" ^
    --vae_dtype bfloat16 ^
    --vae_cache_cpu

:: STEP 2: Text Encoder Caching (NO attention flag)
"%PYTHON_EXE%" "wan_cache_text_encoder_outputs.py" ^
    --t5 "%T5_PATH%" ^
    --fp8_t5 ^
    --batch_size 16

:: STEP 3: Training (REQUIRES attention flag)
"%PYTHON_EXE%" -m accelerate.commands.launch "wan_train_network.py" ^
    --vae "%VAE_PATH%" ^
    --vae_dtype bfloat16 ^
    --vae_cache_cpu ^
    --sage-attn
```

### Available Attention Mechanisms
The training script supports these options (choose one):
- `--sdpa` - Scaled Dot Product Attention (PyTorch native)
- `--flash-attn` - Flash Attention
- `--flash3` - Flash Attention 3
- `--sage-attn` - Sage Attention (recommended if available)
- `--xformers` - xFormers library

Use `--sage-attn` if the logs show "Successfully imported sageattention", otherwise use `--sdpa` as fallback.

## Notes

- Cache generation happens on first epoch and can take significant time
- Once cache files exist, subsequent training runs are much faster
- Cache files are resolution-specific (256, 512, 1024)
- If you modify images or captions, delete cache files to regenerate
- The `--sdpa` flag should not be used in current musubi-tuner scripts