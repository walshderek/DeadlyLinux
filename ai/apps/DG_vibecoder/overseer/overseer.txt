=== OVERSEER MANIFEST START ===

=== FILE START: DG_vibecoder/test/ball.txt ===

=== FILE END: DG_vibecoder/test/ball.txt ===

=== FILE START: DG_vibecoder.py ===
#!/usr/bin/env python3
import os
import sys
import shutil
import subprocess
from pathlib import Path

# CONFIG
ROOT = Path(__file__).resolve().parent
OVERSEER_FILE = ROOT / "overseer" / "overseer.txt"
MODULES_DIR = ROOT / "modules"
LOCAL_REPO = Path("/mnt/c/Users/seanf/Documents/GitHub/deadlygraphics")

# UTILS
def run(cmd, cwd=None):
    print(f"[DEBUG] RUN: {cmd}")
    subprocess.run(cmd, cwd=cwd, text=True, check=False)

def sync_to_github(message):
    print("[SYNC] Syncing local vibecoder -> GitHub mirror...")
    # Copy files
    for item in ROOT.iterdir():
        if item.name in ["logs", "__pycache__", ".git", "apps_managed"]: continue
        dest = LOCAL_REPO / "ai/apps/DG_vibecoder" / item.name
        if item.is_dir():
            if dest.exists(): shutil.rmtree(dest)
            shutil.copytree(item, dest)
        else:
            shutil.copy2(item, dest)
    
    # Push
    run(["git", "add", "."], cwd=LOCAL_REPO)
    run(["git", "commit", "-m", message], cwd=LOCAL_REPO)
    run(["git", "push", "origin", "main"], cwd=LOCAL_REPO)
    print("[SYNC] Git push complete.")

# LOGIC
def parse_patch_blocks(text):
    blocks = []
    current = None
    for line in text.splitlines():
        if line.startswith("=== PATCH START:"):
            filename = line.split(":")[1].strip().split("===")[0].strip()
            current = {"file": filename, "content": []}
        elif line.startswith("=== PATCH END"):
            if current: blocks.append(current); current = None
        elif current:
            current["content"].append(line)
    return blocks

def implement_patches():
    if not OVERSEER_FILE.exists(): return print("[ERROR] overseer.txt missing")
    txt = OVERSEER_FILE.read_text(encoding="utf-8")
    patches = parse_patch_blocks(txt)
    print(f"[INFO] Found {len(patches)} patches.")
    for patch in patches:
        target = ROOT / patch["file"]
        print(f"[OK] Overwriting: {target}")
        target.parent.mkdir(parents=True, exist_ok=True)
        target.write_text("\n".join(patch["content"]), encoding="utf-8")
    sync_to_github("Vibecoder auto-sync (implement)")

def generate_manifest():
    lines = ["=== OVERSEER MANIFEST START ===\n"]
    for p in sorted(ROOT.rglob("*")):
        if p.is_dir() or "overseer.txt" in str(p) or "apps_managed" in str(p): continue
        rel = p.relative_to(ROOT)
        try: text = p.read_text(encoding="utf-8")
        except: text = "<BINARY FILE>"
        lines.append(f"=== FILE START: {rel} ===\n{text}\n=== FILE END: {rel} ===\n")
    lines.append("=== OVERSEER MANIFEST END ===\n")
    (ROOT / "overseer").mkdir(exist_ok=True)
    OVERSEER_FILE.write_text("\n".join(lines), encoding="utf-8")
    sync_to_github("Vibecoder auto-sync (dump)")

# MAIN
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: DG_vibecoder.py [overseer-implement | overseer-dump | install-suite]")
        sys.exit(1)
    
    mode = sys.argv[1]
    
    if mode == "overseer-implement": implement_patches()
    elif mode == "overseer-dump": generate_manifest()
    elif mode == "install-suite":
        if len(sys.argv) < 3: 
            print("[ERROR] Usage: install-suite <name>")
            sys.exit(1)
        suite = sys.argv[2]
        script = MODULES_DIR / "DG_app_manager.py"
        print(f"[VIBECODER] Dispatching Suite: {suite}")
        subprocess.run([sys.executable, str(script), "--suite", suite])
    else:
        print(f"[ERROR] Unknown mode: {mode}")

=== FILE END: DG_vibecoder.py ===

=== FILE START: docs/architecture.md ===
# Deadly Graphics Architecture

## High-Level Components  
- **DG_vibecoder** â€” automation engine  
- **Overseer system** â€” patch generator + manifest manager  
- **GitHub Mirror** â€” canonical repository state  
- **Docs Suite** â€” auto-generated & AI-updated  

---

## MARCO (Dump)
Produces a complete snapshot of the DG_vibecoder environment:  
- All files  
- Metadata  
- Structure  
- Instructions for patch insertion  

---

## POLO (Implement)
Reads overseer patches and updates the actual codebase:  
- File rewrites  
- Insert-after ops  
- Replace ops  
- New file creation  

POLO is deterministic and safe.  

---

## Principles  
- No manual editing of core components  
- Overseer is the single source of truth  
- Vibecoder must always remain operable  
- Every update is reversible  

This file will expand automatically as new subsystems are added.
=== FILE END: docs/architecture.md ===

=== FILE START: docs/deadlygraphics_overview.md ===
# Deadly Graphics Suite â€” System Overview  
*(Brand typeface: **Lobster**)  

## Purpose  
Deadly Graphics (DG) is an evolving suite of AI-driven development tools  
designed for automation, self-maintaining codebases, and rich documentation.  

DG provides:  
- Intelligent developer workflows  
- Repo-wide automation via MARCO/POLO  
- Cross-platform WSL/Windows syncing  
- Tools that document and modify themselves  

DG aims to feel like a *living code ecosystem*.  

---

## DG_vibecoder's Role  
Vibecoder is the automation heart of DG.  
It handles:  
- Patch-based code editing  
- Full repository snapshots  
- GitHub mirroring  
- Safe reversible updates  
- Self-maintained documentation  

Developers **never edit core tools manually** â€” the overseer manages everything.

---

## Branding Notes  
- Typeface: **Lobster**  
- Visual identity: curves, playful strength, neon undertones  
- Tone: bold, technical, alive  

---

## Roadmap (High-Level)  
- Auto-generated architecture graphs  
- Visual dependency mapping  
- Workflow state-machine diagrams  
- Versioning timelines  
- Multi-app orchestration layer  

This document is the beginning of the official DG documentation tree.
=== FILE END: docs/deadlygraphics_overview.md ===

=== FILE START: docs/diagrams/architecture.svg ===
<svg xmlns="http://www.w3.org/2000/svg" width="1000" height="650">
  <rect width="1000" height="650" fill="#000"/>
  <text x="40" y="50" fill="#0ff" font-size="30" font-family="monospace">
    Deadly Graphics â€” Architecture Diagram
  </text>

  <!-- Vibecoder -->
  <rect x="60" y="120" width="300" height="120"
        fill="#111" stroke="#0ff" stroke-width="2"/>
  <text x="80" y="160" fill="#0ff" font-size="18" font-family="monospace">
    DG_vibecoder
  </text>
  <text x="80" y="185" fill="#0ff" font-size="15" font-family="monospace">
    Automation Engine
  </text>

  <!-- Overseer -->
  <rect x="400" y="120" width="300" height="120"
        fill="#111" stroke="#f0f" stroke-width="2"/>
  <text x="420" y="160" fill="#f0f" font-size="18" font-family="monospace">
    Overseer System
  </text>
  <text x="420" y="185" fill="#f0f" font-size="15" font-family="monospace">
    Snapshot & Patch Manager
  </text>

  <!-- GitHub Mirror -->
  <rect x="740" y="120" width="220" height="120"
        fill="#111" stroke="#0f0" stroke-width="2"/>
  <text x="760" y="160" fill="#0f0" font-size="18" font-family="monospace">
    GitHub Mirror
  </text>
  <text x="760" y="185" fill="#0f0" font-size="15" font-family="monospace">
    Canonical Repo State
  </text>

  <!-- Arrows -->
  <line x1="360" y1="180" x2="400" y2="180"
        stroke="#fff" stroke-width="2"/>
  <line x1="700" y1="180" x2="740" y2="180"
        stroke="#fff" stroke-width="2"/>

  <!-- Description -->
  <text x="60" y="300" fill="#fff" font-size="18" font-family="monospace">
    DG behaves like a living, self-maintaining code ecosystem.
  </text>
</svg>
=== FILE END: docs/diagrams/architecture.svg ===

=== FILE START: docs/diagrams/workflow.svg ===
<svg xmlns="http://www.w3.org/2000/svg" width="900" height="500">
  <rect width="900" height="500" fill="#111"/>
  <text x="50" y="60" fill="#0f0" font-size="28" font-family="monospace">Deadly Graphics â€” Workflow Diagram</text>

  <text x="50" y="140" fill="#0ff" font-size="20" font-family="monospace">Developer â†’ MARCO â†’ LLM â†’ POLO â†’ GitHub</text>

  <rect x="40" y="180" width="200" height="80" fill="#222" stroke="#0f0"/>
  <text x="55" y="225" fill="#0f0" font-size="16" font-family="monospace">Developer Runs:</text>
  <text x="55" y="245" fill="#0f0" font-size="16" font-family="monospace">overseer-dump</text>

  <rect x="260" y="180" width="200" height="80" fill="#222" stroke="#0ff"/>
  <text x="275" y="225" fill="#0ff" font-size="16" font-family="monospace">MARCO Snapshot</text>

  <rect x="480" y="180" width="200" height="80" fill="#222" stroke="#f0f"/>
  <text x="495" y="225" fill="#f0f" font-size="16" font-family="monospace">LLM Writes PATCHES</text>

  <rect x="700" y="180" width="200" height="80" fill="#222" stroke="#ff0"/>
  <text x="715" y="225" fill="#ff0" font-size="16" font-family="monospace">POLO Applies</text>

  <text x="50" y="330" fill="#fff" font-size="18" font-family="monospace">Result â†’ Updated codebase + auto GitHub sync</text>
</svg>
=== FILE END: docs/diagrams/workflow.svg ===

=== FILE START: docs/index.md ===
# Deadly Graphics Documentation Index

Welcome to the DG documentation tree.  
This index provides a human-friendly entry point into the system.

---

## Core Documents  
- **Deadly Graphics Overview**  
  â†’ `docs/deadlygraphics_overview.md`  

- **Architecture Overview**  
  â†’ `docs/architecture.md`  

- **Workflow Guide**  
  â†’ `docs/workflow.md`  

- **Auto-Generated Diagrams**  
  â†’ `docs/diagrams/workflow.svg`  
  â†’ `docs/diagrams/architecture.svg`  

---

## Purpose of This Index  
As DG evolves, this index will grow to include:  
- API references  
- Module trees  
- Dependency graphs  
- Version histories  
- Developer onboarding docs  

DG is designed to be self-explanatory and self-maintaining.  
This file is your anchor point.
=== FILE END: docs/index.md ===

=== FILE START: hello_from_vibecoder.txt ===
Hello world! This file was created by DG_vibecoder.

=== FILE END: hello_from_vibecoder.txt ===

=== FILE START: logs/overseer_response.txt ===
# DIRECTLY CREATE THE BOOTSTRAP INSTALLER
cat << 'EOF' > install_deadlygraphics.py
#!/usr/bin/env python3
"""
THE GOLDEN KEY: install_deadlygraphics.py
Run this on a fresh WSL instance to bootstrap the entire Deadly Graphics ecosystem.
"""
import os
import sys
import subprocess
import shutil
from pathlib import Path

# CONFIGURATION
REPO_URL = "https://github.com/walshderek/deadlygraphics.git"
# Default install location
INSTALL_DIR = Path.home() / "workspace" / "deadlygraphics"
MODELS_SRC = Path("/mnt/c/AI/models")

def step(msg):
    print(f"\n[+] {msg}")

def run(cmd, cwd=None):
    # shell=True allows using pipes and env vars easily
    subprocess.run(cmd, shell=True, check=True, cwd=cwd)

def check_requirements():
    step("Checking System Requirements...")
    
    # 1. Check WSL
    if "microsoft" in os.uname().release.lower():
        print("[INFO] WSL Environment detected.")
    else:
        print("[WARN] Not running on WSL? Proceed with caution.")
    
    # 2. Check Git
    if not shutil.which("git"):
        step("Installing Git...")
        run("sudo apt update && sudo apt install -y git")
    else:
        print("[OK] Git detected.")
    
    # 3. Check Python venv (Crucial for Ubuntu/Debian)
    try:
        import ensurepip
    except ImportError:
        step("Installing Python venv & dev tools...")
        run("sudo apt update && sudo apt install -y python3-venv python3-full build-essential")

def install_uv():
    step("Installing UV (The Accelerator)...")
    if shutil.which("uv"):
        print("[OK] UV is already installed.")
        return
    
    try:
        run("curl -LsSf https://astral.sh/uv/install.sh | sh")
        # Add to path for this session so we can use it immediately
        os.environ["PATH"] += f":{Path.home()}/.local/bin"
    except Exception as e:
        print(f"[WARN] UV install failed: {e}")

def clone_repo():
    step(f"Cloning DeadlyGraphics to {INSTALL_DIR}...")
    if INSTALL_DIR.exists():
        print("[INFO] Directory exists. Pulling latest changes...")
        run("git pull", cwd=INSTALL_DIR)
    else:
        INSTALL_DIR.parent.mkdir(parents=True, exist_ok=True)
        run(f"git clone {REPO_URL} {INSTALL_DIR}")

def bootstrap_vibecoder():
    step("Bootstrapping Vibecoder Suite...")
    
    # Path to the Vibecoder orchestrator
    vibecoder_script = INSTALL_DIR / "ai" / "apps" / "DG_vibecoder" / "DG_vibecoder.py"
    
    if not vibecoder_script.exists():
        print(f"[ERROR] Could not find Vibecoder at: {vibecoder_script}")
        print("      Did the repo structure change?")
        sys.exit(1)
        
    print(f"[EXEC] Running: install-suite DG_AI")
    # Use the same python interpreter to run the child script
    run(f"{sys.executable} {vibecoder_script} install-suite DG_AI")

def check_model_mount():
    step("Verifying Model Storage...")
    if MODELS_SRC.exists():
        print(f"[OK] C: Drive Models found at: {MODELS_SRC}")
        print("      (Linking logic will run in Phase 4)")
    else:
        print(f"[WARN] Could not find models at {MODELS_SRC}")
        print("      Ensure your Windows drive is mounted at /mnt/c")

def main():
    print("=================================================")
    print("ðŸ’Ž DEADLY GRAPHICS â€” BOOTSTRAP INSTALLER ðŸ’Ž")
    print("=================================================")
    
    check_requirements()
    install_uv()
    clone_repo()
    bootstrap_vibecoder()
    check_model_mount()
    
    print("")
    print("=================================================")
    print("âœ… SYSTEM READY. WELCOME TO THE MACHINE.")
    print("=================================================")

if __name__ == "__main__":
    main()
EOF

# Make it executable
chmod +x install_deadlygraphics.py

# RUN IT TO VERIFY PHASE 3
python3 install_deadlygraphics.py
=== FILE END: logs/overseer_response.txt ===

=== FILE START: manifest.json ===
{
  "app_name": "DG_vibecoder",
  "scan_time": "2025-12-11 21:21:17",
  "python_version_detected": "3.12.3",
  "detected_imports": [],
  "config_files": [],
  "gpu_required": false,
  "suggested_system_packages": [],
  "suggested_pypi_packages": []
}
=== FILE END: manifest.json ===

=== FILE START: modules/DG_app_manager.py ===
#!/usr/bin/env python3
import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Try import scanner
try: from . import DG_dependency_scanner
except ImportError: import DG_dependency_scanner

# SUITES
DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

class AppManager:
    def __init__(self, root):
        self.apps_root = root
        self.uv = shutil.which("uv")

    def ensure_uv(self):
        if self.uv: return
        print("[INSTALL] Installing uv...")
        subprocess.run("curl -LsSf https://astral.sh/uv/install.sh | sh", shell=True, check=True)
        self.uv = shutil.which("uv")

    def clone_repo(self, repo_url, app_name):
        target = self.apps_root / app_name
        if target.exists():
            print(f"[UPDATE] git pull for {app_name}...")
            subprocess.run(["git", "pull"], cwd=target, check=False)
        else:
            print(f"[CLONE] {app_name}...")
            subprocess.run(["git", "clone", repo_url, str(target)], check=True)
        
        # FIX: Kohya requires submodules
        if app_name == "Kohya_ss":
            print("[SUBMODULE] Init for Kohya...")
            subprocess.run(["git", "submodule", "update", "--init", "--recursive"], cwd=target, check=False)
        return target

    def install_deps(self, target):
        print(f"[INSTALL] Dependencies for {target.name}...")
        
        # Strategy 1: Fast (UV)
        try:
            subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)
        except subprocess.CalledProcessError:
            print(f"[WARN] 'uv pip' failed. Attempting standard pip fallback...")
            # Strategy 2: Compat (Standard Pip via UV venv)
            subprocess.run(["uv", "pip", "install", "pip"], cwd=target, check=True)
            subprocess.run(["uv", "run", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)

    def scan_app(self, target):
        print("[SCAN] Generating manifest...")
        try:
            data = DG_dependency_scanner.scan_directory(target)
            with open(target / "manifest.json", "w") as f: json.dump(data, f, indent=2)
            print("[SUCCESS] Manifest saved.")
        except Exception as e:
            print(f"[WARN] Scanner skipped: {e}")

    def deploy(self, name, url):
        print(f"\n=== DEPLOYING: {name} ===")
        self.ensure_uv()
        target = self.clone_repo(url, name)
        
        print(f"[VENV] Creating venv for {name}...")
        subprocess.run(["uv", "venv", ".venv"], cwd=target, check=True)
        
        if (target / "requirements.txt").exists():
            self.install_deps(target)
        
        self.scan_app(target)
        print(f"[SUCCESS] {name} deployed.")

    def install_suite(self, suite):
        if suite not in DG_SUITES: return print(f"[ERROR] Unknown suite: {suite}")
        print(f"*** SUITE: {suite} ***")
        for name, url in DG_SUITES[suite].items():
            try: self.deploy(name, url)
            except Exception as e: print(f"[FAIL] {name}: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 3: sys.exit(1)
    # Fix path logic to ensure apps_managed is in correct spot
    root = Path(__file__).resolve().parent.parent / "apps_managed"
    root.mkdir(exist_ok=True)
    mgr = AppManager(root)
    
    if sys.argv[1] == "--suite": mgr.install_suite(sys.argv[2])
    else: mgr.deploy(sys.argv[1], sys.argv[2])

=== FILE END: modules/DG_app_manager.py ===

=== FILE START: modules/DG_dependency_scanner.py ===
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
=== FILE END: modules/DG_dependency_scanner.py ===

=== FILE START: modules/DG_vibecoder.py:Zone.Identifier ===
[ZoneTransfer]
ZoneId=3 
=== FILE END: modules/DG_vibecoder.py:Zone.Identifier ===

=== FILE START: modules/DG_vibecoder_github_push.py ===
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

=== FILE END: modules/DG_vibecoder_github_push.py ===

=== FILE START: modules/__pycache__/DG_dependency_scanner.cpython-312.pyc ===
<BINARY FILE>
=== FILE END: modules/__pycache__/DG_dependency_scanner.cpython-312.pyc ===

=== FILE START: modules/core.py ===
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
=== FILE END: modules/core.py ===

=== FILE START: modules/logs/20251211_213456/history/overseer_adjust.txt ===
=== OVERSEER SNAPSHOT ===
generated=2025-12-11 21:34:56
target_folder=/home/seanf/workspace/deadlygraphics/ai/apps/DG_vibecoder/modules

=== FILES ===
=== FILE START: DG_app_manager.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_app_manager.py
Part of the Deadly Graphics Suite.

Purpose:
  Orchestrates the deployment of external AI applications (ComfyUI, OneTrainer, etc.).
  Implements the "Onion Model":
  - Layer 1: Git clone / Pull
  - Layer 2: UV-accelerated Virtual Environment
  - Layer 3: Dependency Installation
  - Layer 4: Integration (Scanner + Manifest)
"""

import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Import our scanner
try:
    from . import DG_dependency_scanner
except ImportError:
    import DG_dependency_scanner

# ============================================================
# CONFIGURATION: SUITES & PRESETS
# ============================================================

UV_INSTALL_SCRIPT = "curl -LsSf https://astral.sh/uv/install.sh | sh"

DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

# ============================================================
# APP MANAGER LOGIC
# ============================================================

class AppManager:
    def __init__(self, apps_root: Path):
        self.apps_root = apps_root
        self.uv_bin = self._find_uv()

    def _find_uv(self):
        """Locates 'uv' executable or returns None."""
        return shutil.which("uv")

    def ensure_uv(self):
        """Installs uv if missing."""
        if self.uv_bin:
            # print(f"[OK] uv detected: {self.uv_bin}")
            return
        
        print("[INSTALL] Installing uv (The Accelerator)...")
        try:
            subprocess.run(UV_INSTALL_SCRIPT, shell=True, check=True)
            print("[SUCCESS] uv installed.")
            self.uv_bin = shutil.which("uv") 
        except subprocess.CalledProcessError as e:
            print(f"[ERROR] Failed to install uv: {e}")
            raise

    def clone_repo(self, repo_url: str, app_name: str):
        """Clones a repository into apps_root/app_name, or pulls if exists."""
        target_dir = self.apps_root / app_name
        
        if target_dir.exists():
            print(f"[INFO] App directory exists: {target_dir}")
            print(f"[UPDATE] Attempting git pull...")
            # Non-blocking pull (check=False) as per architecture
            try:
                subprocess.run(["git", "pull"], cwd=target_dir, check=False)
            except Exception as e:
                print(f"[WARN] Git pull failed, continuing deployment: {e}")
            return target_dir
            
        print(f"[CLONE] Cloning {app_name} from {repo_url}...")
        subprocess.run(["git", "clone", repo_url, str(target_dir)], check=True)
        return target_dir

    def create_venv(self, app_dir: Path):
        """Creates a venv using uv."""
        venv_dir = app_dir / ".venv"
        if venv_dir.exists():
            return venv_dir

        print(f"[VENV] Creating venv for {app_dir.name} using uv...")
        subprocess.run(["uv", "venv", ".venv"], cwd=app_dir, check=True)
        return venv_dir

    def install_deps(self, app_dir: Path):
        """Installs dependencies from requirements.txt using uv pip."""
        req_file = app_dir / "requirements.txt"
        if not req_file.exists():
            print(f"[WARN] No requirements.txt found in {app_dir.name}")
            return

        print(f"[INSTALL] Installing dependencies for {app_dir.name}...")
        # uv auto-detects .venv in cwd
        subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=app_dir, check=True)

    def scan_app(self, app_dir: Path):
        """Runs the DG_dependency_scanner on the new app."""
        print(f"[SCAN] Generating manifest for {app_dir.name}...")
        data = DG_dependency_scanner.scan_directory(app_dir)
        
        manifest_path = app_dir / "manifest.json"
        with open(manifest_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2)
        print(f"[SUCCESS] Manifest saved.")

    def deploy_app(self, app_name: str, repo_url: str):
        """Full deployment pipeline for a single app."""
        print(f"\n=== DEPLOYING: {app_name} ===")
        self.ensure_uv()
        
        app_dir = self.clone_repo(repo_url, app_name)
        self.create_venv(app_dir)
        self.install_deps(app_dir)
        self.scan_app(app_dir)
        
        print(f"=== DEPLOYMENT COMPLETE: {app_name} ===\n")

    def install_suite(self, suite_name: str):
        """Orchestrates installation of a named suite."""
        if suite_name not in DG_SUITES:
            print(f"[ERROR] Suite '{suite_name}' not defined in DG_app_manager.")
            print(f"Available suites: {list(DG_SUITES.keys())}")
            return

        print(f"*******************************************")
        print(f"*** STARTING SUITE INSTALL: {suite_name}")
        print(f"*******************************************")
        
        apps = DG_SUITES[suite_name]
        for name, url in apps.items():
            try:
                self.deploy_app(name, url)
            except Exception as e:
                print(f"[CRITICAL] Failed to deploy {name}: {e}")
                print("Continuing to next app in suite...")
        
        print(f"*******************************************")
        print(f"*** SUITE INSTALL COMPLETE: {suite_name}")
        print(f"*******************************************")

# ============================================================
# CLI ENTRYPOINT
# ============================================================

def main():
    if len(sys.argv) < 3:
        print("Usage:")
        print("  python3 DG_app_manager.py <app_name> <repo_url>")
        print("  python3 DG_app_manager.py --suite <suite_name>")
        return

    # Determine location for managed apps (DG_vibecoder/apps_managed)
    root = Path(__file__).resolve().parent.parent
    apps_managed = root / "apps_managed"
    apps_managed.mkdir(exist_ok=True)
    
    manager = AppManager(apps_managed)

    arg1 = sys.argv[1]
    arg2 = sys.argv[2]

    if arg1 == "--suite":
        manager.install_suite(arg2)
    else:
        # Standard mode: app_name, repo_url
        manager.deploy_app(arg1, arg2)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: core.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
</CONTENT>
=== FILE END ===

=== FILE START: DG_dependency_scanner.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: DG_vibecoder_github_push.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

</CONTENT>
=== FILE END ===

=== SUMMARY ===
total_files=4

=== FILE END: modules/logs/20251211_213456/history/overseer_adjust.txt ===

=== FILE START: modules/logs/20251211_213456/overseer_adjust.txt ===
=== OVERSEER SNAPSHOT ===
generated=2025-12-11 21:34:56
target_folder=/home/seanf/workspace/deadlygraphics/ai/apps/DG_vibecoder/modules

=== FILES ===
=== FILE START: DG_app_manager.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_app_manager.py
Part of the Deadly Graphics Suite.

Purpose:
  Orchestrates the deployment of external AI applications (ComfyUI, OneTrainer, etc.).
  Implements the "Onion Model":
  - Layer 1: Git clone / Pull
  - Layer 2: UV-accelerated Virtual Environment
  - Layer 3: Dependency Installation
  - Layer 4: Integration (Scanner + Manifest)
"""

import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Import our scanner
try:
    from . import DG_dependency_scanner
except ImportError:
    import DG_dependency_scanner

# ============================================================
# CONFIGURATION: SUITES & PRESETS
# ============================================================

UV_INSTALL_SCRIPT = "curl -LsSf https://astral.sh/uv/install.sh | sh"

DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

# ============================================================
# APP MANAGER LOGIC
# ============================================================

class AppManager:
    def __init__(self, apps_root: Path):
        self.apps_root = apps_root
        self.uv_bin = self._find_uv()

    def _find_uv(self):
        """Locates 'uv' executable or returns None."""
        return shutil.which("uv")

    def ensure_uv(self):
        """Installs uv if missing."""
        if self.uv_bin:
            # print(f"[OK] uv detected: {self.uv_bin}")
            return
        
        print("[INSTALL] Installing uv (The Accelerator)...")
        try:
            subprocess.run(UV_INSTALL_SCRIPT, shell=True, check=True)
            print("[SUCCESS] uv installed.")
            self.uv_bin = shutil.which("uv") 
        except subprocess.CalledProcessError as e:
            print(f"[ERROR] Failed to install uv: {e}")
            raise

    def clone_repo(self, repo_url: str, app_name: str):
        """Clones a repository into apps_root/app_name, or pulls if exists."""
        target_dir = self.apps_root / app_name
        
        if target_dir.exists():
            print(f"[INFO] App directory exists: {target_dir}")
            print(f"[UPDATE] Attempting git pull...")
            # Non-blocking pull (check=False) as per architecture
            try:
                subprocess.run(["git", "pull"], cwd=target_dir, check=False)
            except Exception as e:
                print(f"[WARN] Git pull failed, continuing deployment: {e}")
            return target_dir
            
        print(f"[CLONE] Cloning {app_name} from {repo_url}...")
        subprocess.run(["git", "clone", repo_url, str(target_dir)], check=True)
        return target_dir

    def create_venv(self, app_dir: Path):
        """Creates a venv using uv."""
        venv_dir = app_dir / ".venv"
        if venv_dir.exists():
            return venv_dir

        print(f"[VENV] Creating venv for {app_dir.name} using uv...")
        subprocess.run(["uv", "venv", ".venv"], cwd=app_dir, check=True)
        return venv_dir

    def install_deps(self, app_dir: Path):
        """Installs dependencies from requirements.txt using uv pip."""
        req_file = app_dir / "requirements.txt"
        if not req_file.exists():
            print(f"[WARN] No requirements.txt found in {app_dir.name}")
            return

        print(f"[INSTALL] Installing dependencies for {app_dir.name}...")
        # uv auto-detects .venv in cwd
        subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=app_dir, check=True)

    def scan_app(self, app_dir: Path):
        """Runs the DG_dependency_scanner on the new app."""
        print(f"[SCAN] Generating manifest for {app_dir.name}...")
        data = DG_dependency_scanner.scan_directory(app_dir)
        
        manifest_path = app_dir / "manifest.json"
        with open(manifest_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2)
        print(f"[SUCCESS] Manifest saved.")

    def deploy_app(self, app_name: str, repo_url: str):
        """Full deployment pipeline for a single app."""
        print(f"\n=== DEPLOYING: {app_name} ===")
        self.ensure_uv()
        
        app_dir = self.clone_repo(repo_url, app_name)
        self.create_venv(app_dir)
        self.install_deps(app_dir)
        self.scan_app(app_dir)
        
        print(f"=== DEPLOYMENT COMPLETE: {app_name} ===\n")

    def install_suite(self, suite_name: str):
        """Orchestrates installation of a named suite."""
        if suite_name not in DG_SUITES:
            print(f"[ERROR] Suite '{suite_name}' not defined in DG_app_manager.")
            print(f"Available suites: {list(DG_SUITES.keys())}")
            return

        print(f"*******************************************")
        print(f"*** STARTING SUITE INSTALL: {suite_name}")
        print(f"*******************************************")
        
        apps = DG_SUITES[suite_name]
        for name, url in apps.items():
            try:
                self.deploy_app(name, url)
            except Exception as e:
                print(f"[CRITICAL] Failed to deploy {name}: {e}")
                print("Continuing to next app in suite...")
        
        print(f"*******************************************")
        print(f"*** SUITE INSTALL COMPLETE: {suite_name}")
        print(f"*******************************************")

# ============================================================
# CLI ENTRYPOINT
# ============================================================

def main():
    if len(sys.argv) < 3:
        print("Usage:")
        print("  python3 DG_app_manager.py <app_name> <repo_url>")
        print("  python3 DG_app_manager.py --suite <suite_name>")
        return

    # Determine location for managed apps (DG_vibecoder/apps_managed)
    root = Path(__file__).resolve().parent.parent
    apps_managed = root / "apps_managed"
    apps_managed.mkdir(exist_ok=True)
    
    manager = AppManager(apps_managed)

    arg1 = sys.argv[1]
    arg2 = sys.argv[2]

    if arg1 == "--suite":
        manager.install_suite(arg2)
    else:
        # Standard mode: app_name, repo_url
        manager.deploy_app(arg1, arg2)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: core.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
</CONTENT>
=== FILE END ===

=== FILE START: DG_dependency_scanner.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: DG_vibecoder_github_push.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

</CONTENT>
=== FILE END ===

=== SUMMARY ===
total_files=4

=== FILE END: modules/logs/20251211_213456/overseer_adjust.txt ===

=== FILE START: modules/logs/20251211_213516/history/overseer_adjust.txt ===
=== OVERSEER SNAPSHOT ===
generated=2025-12-11 21:35:16
target_folder=/home/seanf/workspace/deadlygraphics/ai/apps/DG_vibecoder/modules

=== FILES ===
=== FILE START: DG_app_manager.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_app_manager.py
Part of the Deadly Graphics Suite.

Purpose:
  Orchestrates the deployment of external AI applications (ComfyUI, OneTrainer, etc.).
  Implements the "Onion Model":
  - Layer 1: Git clone / Pull
  - Layer 2: UV-accelerated Virtual Environment
  - Layer 3: Dependency Installation
  - Layer 4: Integration (Scanner + Manifest)
"""

import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Import our scanner
try:
    from . import DG_dependency_scanner
except ImportError:
    import DG_dependency_scanner

# ============================================================
# CONFIGURATION: SUITES & PRESETS
# ============================================================

UV_INSTALL_SCRIPT = "curl -LsSf https://astral.sh/uv/install.sh | sh"

DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

# ============================================================
# APP MANAGER LOGIC
# ============================================================

class AppManager:
    def __init__(self, apps_root: Path):
        self.apps_root = apps_root
        self.uv_bin = self._find_uv()

    def _find_uv(self):
        """Locates 'uv' executable or returns None."""
        return shutil.which("uv")

    def ensure_uv(self):
        """Installs uv if missing."""
        if self.uv_bin:
            # print(f"[OK] uv detected: {self.uv_bin}")
            return
        
        print("[INSTALL] Installing uv (The Accelerator)...")
        try:
            subprocess.run(UV_INSTALL_SCRIPT, shell=True, check=True)
            print("[SUCCESS] uv installed.")
            self.uv_bin = shutil.which("uv") 
        except subprocess.CalledProcessError as e:
            print(f"[ERROR] Failed to install uv: {e}")
            raise

    def clone_repo(self, repo_url: str, app_name: str):
        """Clones a repository into apps_root/app_name, or pulls if exists."""
        target_dir = self.apps_root / app_name
        
        if target_dir.exists():
            print(f"[INFO] App directory exists: {target_dir}")
            print(f"[UPDATE] Attempting git pull...")
            # Non-blocking pull (check=False) as per architecture
            try:
                subprocess.run(["git", "pull"], cwd=target_dir, check=False)
            except Exception as e:
                print(f"[WARN] Git pull failed, continuing deployment: {e}")
            return target_dir
            
        print(f"[CLONE] Cloning {app_name} from {repo_url}...")
        subprocess.run(["git", "clone", repo_url, str(target_dir)], check=True)
        return target_dir

    def create_venv(self, app_dir: Path):
        """Creates a venv using uv."""
        venv_dir = app_dir / ".venv"
        if venv_dir.exists():
            return venv_dir

        print(f"[VENV] Creating venv for {app_dir.name} using uv...")
        subprocess.run(["uv", "venv", ".venv"], cwd=app_dir, check=True)
        return venv_dir

    def install_deps(self, app_dir: Path):
        """Installs dependencies from requirements.txt using uv pip."""
        req_file = app_dir / "requirements.txt"
        if not req_file.exists():
            print(f"[WARN] No requirements.txt found in {app_dir.name}")
            return

        print(f"[INSTALL] Installing dependencies for {app_dir.name}...")
        # uv auto-detects .venv in cwd
        subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=app_dir, check=True)

    def scan_app(self, app_dir: Path):
        """Runs the DG_dependency_scanner on the new app."""
        print(f"[SCAN] Generating manifest for {app_dir.name}...")
        data = DG_dependency_scanner.scan_directory(app_dir)
        
        manifest_path = app_dir / "manifest.json"
        with open(manifest_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2)
        print(f"[SUCCESS] Manifest saved.")

    def deploy_app(self, app_name: str, repo_url: str):
        """Full deployment pipeline for a single app."""
        print(f"\n=== DEPLOYING: {app_name} ===")
        self.ensure_uv()
        
        app_dir = self.clone_repo(repo_url, app_name)
        self.create_venv(app_dir)
        self.install_deps(app_dir)
        self.scan_app(app_dir)
        
        print(f"=== DEPLOYMENT COMPLETE: {app_name} ===\n")

    def install_suite(self, suite_name: str):
        """Orchestrates installation of a named suite."""
        if suite_name not in DG_SUITES:
            print(f"[ERROR] Suite '{suite_name}' not defined in DG_app_manager.")
            print(f"Available suites: {list(DG_SUITES.keys())}")
            return

        print(f"*******************************************")
        print(f"*** STARTING SUITE INSTALL: {suite_name}")
        print(f"*******************************************")
        
        apps = DG_SUITES[suite_name]
        for name, url in apps.items():
            try:
                self.deploy_app(name, url)
            except Exception as e:
                print(f"[CRITICAL] Failed to deploy {name}: {e}")
                print("Continuing to next app in suite...")
        
        print(f"*******************************************")
        print(f"*** SUITE INSTALL COMPLETE: {suite_name}")
        print(f"*******************************************")

# ============================================================
# CLI ENTRYPOINT
# ============================================================

def main():
    if len(sys.argv) < 3:
        print("Usage:")
        print("  python3 DG_app_manager.py <app_name> <repo_url>")
        print("  python3 DG_app_manager.py --suite <suite_name>")
        return

    # Determine location for managed apps (DG_vibecoder/apps_managed)
    root = Path(__file__).resolve().parent.parent
    apps_managed = root / "apps_managed"
    apps_managed.mkdir(exist_ok=True)
    
    manager = AppManager(apps_managed)

    arg1 = sys.argv[1]
    arg2 = sys.argv[2]

    if arg1 == "--suite":
        manager.install_suite(arg2)
    else:
        # Standard mode: app_name, repo_url
        manager.deploy_app(arg1, arg2)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: core.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
</CONTENT>
=== FILE END ===

=== FILE START: DG_dependency_scanner.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: DG_vibecoder_github_push.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

</CONTENT>
=== FILE END ===

=== SUMMARY ===
total_files=4

=== FILE END: modules/logs/20251211_213516/history/overseer_adjust.txt ===

=== FILE START: modules/logs/20251211_213516/overseer_adjust.txt ===
=== OVERSEER SNAPSHOT ===
generated=2025-12-11 21:35:16
target_folder=/home/seanf/workspace/deadlygraphics/ai/apps/DG_vibecoder/modules

=== FILES ===
=== FILE START: DG_app_manager.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_app_manager.py
Part of the Deadly Graphics Suite.

Purpose:
  Orchestrates the deployment of external AI applications (ComfyUI, OneTrainer, etc.).
  Implements the "Onion Model":
  - Layer 1: Git clone / Pull
  - Layer 2: UV-accelerated Virtual Environment
  - Layer 3: Dependency Installation
  - Layer 4: Integration (Scanner + Manifest)
"""

import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Import our scanner
try:
    from . import DG_dependency_scanner
except ImportError:
    import DG_dependency_scanner

# ============================================================
# CONFIGURATION: SUITES & PRESETS
# ============================================================

UV_INSTALL_SCRIPT = "curl -LsSf https://astral.sh/uv/install.sh | sh"

DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

# ============================================================
# APP MANAGER LOGIC
# ============================================================

class AppManager:
    def __init__(self, apps_root: Path):
        self.apps_root = apps_root
        self.uv_bin = self._find_uv()

    def _find_uv(self):
        """Locates 'uv' executable or returns None."""
        return shutil.which("uv")

    def ensure_uv(self):
        """Installs uv if missing."""
        if self.uv_bin:
            # print(f"[OK] uv detected: {self.uv_bin}")
            return
        
        print("[INSTALL] Installing uv (The Accelerator)...")
        try:
            subprocess.run(UV_INSTALL_SCRIPT, shell=True, check=True)
            print("[SUCCESS] uv installed.")
            self.uv_bin = shutil.which("uv") 
        except subprocess.CalledProcessError as e:
            print(f"[ERROR] Failed to install uv: {e}")
            raise

    def clone_repo(self, repo_url: str, app_name: str):
        """Clones a repository into apps_root/app_name, or pulls if exists."""
        target_dir = self.apps_root / app_name
        
        if target_dir.exists():
            print(f"[INFO] App directory exists: {target_dir}")
            print(f"[UPDATE] Attempting git pull...")
            # Non-blocking pull (check=False) as per architecture
            try:
                subprocess.run(["git", "pull"], cwd=target_dir, check=False)
            except Exception as e:
                print(f"[WARN] Git pull failed, continuing deployment: {e}")
            return target_dir
            
        print(f"[CLONE] Cloning {app_name} from {repo_url}...")
        subprocess.run(["git", "clone", repo_url, str(target_dir)], check=True)
        return target_dir

    def create_venv(self, app_dir: Path):
        """Creates a venv using uv."""
        venv_dir = app_dir / ".venv"
        if venv_dir.exists():
            return venv_dir

        print(f"[VENV] Creating venv for {app_dir.name} using uv...")
        subprocess.run(["uv", "venv", ".venv"], cwd=app_dir, check=True)
        return venv_dir

    def install_deps(self, app_dir: Path):
        """Installs dependencies from requirements.txt using uv pip."""
        req_file = app_dir / "requirements.txt"
        if not req_file.exists():
            print(f"[WARN] No requirements.txt found in {app_dir.name}")
            return

        print(f"[INSTALL] Installing dependencies for {app_dir.name}...")
        # uv auto-detects .venv in cwd
        subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=app_dir, check=True)

    def scan_app(self, app_dir: Path):
        """Runs the DG_dependency_scanner on the new app."""
        print(f"[SCAN] Generating manifest for {app_dir.name}...")
        data = DG_dependency_scanner.scan_directory(app_dir)
        
        manifest_path = app_dir / "manifest.json"
        with open(manifest_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2)
        print(f"[SUCCESS] Manifest saved.")

    def deploy_app(self, app_name: str, repo_url: str):
        """Full deployment pipeline for a single app."""
        print(f"\n=== DEPLOYING: {app_name} ===")
        self.ensure_uv()
        
        app_dir = self.clone_repo(repo_url, app_name)
        self.create_venv(app_dir)
        self.install_deps(app_dir)
        self.scan_app(app_dir)
        
        print(f"=== DEPLOYMENT COMPLETE: {app_name} ===\n")

    def install_suite(self, suite_name: str):
        """Orchestrates installation of a named suite."""
        if suite_name not in DG_SUITES:
            print(f"[ERROR] Suite '{suite_name}' not defined in DG_app_manager.")
            print(f"Available suites: {list(DG_SUITES.keys())}")
            return

        print(f"*******************************************")
        print(f"*** STARTING SUITE INSTALL: {suite_name}")
        print(f"*******************************************")
        
        apps = DG_SUITES[suite_name]
        for name, url in apps.items():
            try:
                self.deploy_app(name, url)
            except Exception as e:
                print(f"[CRITICAL] Failed to deploy {name}: {e}")
                print("Continuing to next app in suite...")
        
        print(f"*******************************************")
        print(f"*** SUITE INSTALL COMPLETE: {suite_name}")
        print(f"*******************************************")

# ============================================================
# CLI ENTRYPOINT
# ============================================================

def main():
    if len(sys.argv) < 3:
        print("Usage:")
        print("  python3 DG_app_manager.py <app_name> <repo_url>")
        print("  python3 DG_app_manager.py --suite <suite_name>")
        return

    # Determine location for managed apps (DG_vibecoder/apps_managed)
    root = Path(__file__).resolve().parent.parent
    apps_managed = root / "apps_managed"
    apps_managed.mkdir(exist_ok=True)
    
    manager = AppManager(apps_managed)

    arg1 = sys.argv[1]
    arg2 = sys.argv[2]

    if arg1 == "--suite":
        manager.install_suite(arg2)
    else:
        # Standard mode: app_name, repo_url
        manager.deploy_app(arg1, arg2)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: core.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
</CONTENT>
=== FILE END ===

=== FILE START: DG_dependency_scanner.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: DG_vibecoder_github_push.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

</CONTENT>
=== FILE END ===

=== SUMMARY ===
total_files=4

=== FILE END: modules/logs/20251211_213516/overseer_adjust.txt ===

=== FILE START: modules/logs/20251211_213730/history/overseer_adjust.txt ===
=== OVERSEER SNAPSHOT ===
generated=2025-12-11 21:37:30
target_folder=/home/seanf/workspace/deadlygraphics/ai/apps/DG_vibecoder/modules

=== FILES ===
=== FILE START: DG_app_manager.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_app_manager.py
Part of the Deadly Graphics Suite.

Purpose:
  Orchestrates the deployment of external AI applications (ComfyUI, OneTrainer, etc.).
  Implements the "Onion Model":
  - Layer 1: Git clone / Pull
  - Layer 2: UV-accelerated Virtual Environment
  - Layer 3: Dependency Installation
  - Layer 4: Integration (Scanner + Manifest)
"""

import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Import our scanner
try:
    from . import DG_dependency_scanner
except ImportError:
    import DG_dependency_scanner

# ============================================================
# CONFIGURATION: SUITES & PRESETS
# ============================================================

UV_INSTALL_SCRIPT = "curl -LsSf https://astral.sh/uv/install.sh | sh"

DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

# ============================================================
# APP MANAGER LOGIC
# ============================================================

class AppManager:
    def __init__(self, apps_root: Path):
        self.apps_root = apps_root
        self.uv_bin = self._find_uv()

    def _find_uv(self):
        """Locates 'uv' executable or returns None."""
        return shutil.which("uv")

    def ensure_uv(self):
        """Installs uv if missing."""
        if self.uv_bin:
            # print(f"[OK] uv detected: {self.uv_bin}")
            return
        
        print("[INSTALL] Installing uv (The Accelerator)...")
        try:
            subprocess.run(UV_INSTALL_SCRIPT, shell=True, check=True)
            print("[SUCCESS] uv installed.")
            self.uv_bin = shutil.which("uv") 
        except subprocess.CalledProcessError as e:
            print(f"[ERROR] Failed to install uv: {e}")
            raise

    def clone_repo(self, repo_url: str, app_name: str):
        """Clones a repository into apps_root/app_name, or pulls if exists."""
        target_dir = self.apps_root / app_name
        
        if target_dir.exists():
            print(f"[INFO] App directory exists: {target_dir}")
            print(f"[UPDATE] Attempting git pull...")
            # Non-blocking pull (check=False) as per architecture
            try:
                subprocess.run(["git", "pull"], cwd=target_dir, check=False)
            except Exception as e:
                print(f"[WARN] Git pull failed, continuing deployment: {e}")
            return target_dir
            
        print(f"[CLONE] Cloning {app_name} from {repo_url}...")
        subprocess.run(["git", "clone", repo_url, str(target_dir)], check=True)
        return target_dir

    def create_venv(self, app_dir: Path):
        """Creates a venv using uv."""
        venv_dir = app_dir / ".venv"
        if venv_dir.exists():
            return venv_dir

        print(f"[VENV] Creating venv for {app_dir.name} using uv...")
        subprocess.run(["uv", "venv", ".venv"], cwd=app_dir, check=True)
        return venv_dir

    def install_deps(self, app_dir: Path):
        """Installs dependencies from requirements.txt using uv pip."""
        req_file = app_dir / "requirements.txt"
        if not req_file.exists():
            print(f"[WARN] No requirements.txt found in {app_dir.name}")
            return

        print(f"[INSTALL] Installing dependencies for {app_dir.name}...")
        # uv auto-detects .venv in cwd
        subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=app_dir, check=True)

    def scan_app(self, app_dir: Path):
        """Runs the DG_dependency_scanner on the new app."""
        print(f"[SCAN] Generating manifest for {app_dir.name}...")
        data = DG_dependency_scanner.scan_directory(app_dir)
        
        manifest_path = app_dir / "manifest.json"
        with open(manifest_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2)
        print(f"[SUCCESS] Manifest saved.")

    def deploy_app(self, app_name: str, repo_url: str):
        """Full deployment pipeline for a single app."""
        print(f"\n=== DEPLOYING: {app_name} ===")
        self.ensure_uv()
        
        app_dir = self.clone_repo(repo_url, app_name)
        self.create_venv(app_dir)
        self.install_deps(app_dir)
        self.scan_app(app_dir)
        
        print(f"=== DEPLOYMENT COMPLETE: {app_name} ===\n")

    def install_suite(self, suite_name: str):
        """Orchestrates installation of a named suite."""
        if suite_name not in DG_SUITES:
            print(f"[ERROR] Suite '{suite_name}' not defined in DG_app_manager.")
            print(f"Available suites: {list(DG_SUITES.keys())}")
            return

        print(f"*******************************************")
        print(f"*** STARTING SUITE INSTALL: {suite_name}")
        print(f"*******************************************")
        
        apps = DG_SUITES[suite_name]
        for name, url in apps.items():
            try:
                self.deploy_app(name, url)
            except Exception as e:
                print(f"[CRITICAL] Failed to deploy {name}: {e}")
                print("Continuing to next app in suite...")
        
        print(f"*******************************************")
        print(f"*** SUITE INSTALL COMPLETE: {suite_name}")
        print(f"*******************************************")

# ============================================================
# CLI ENTRYPOINT
# ============================================================

def main():
    if len(sys.argv) < 3:
        print("Usage:")
        print("  python3 DG_app_manager.py <app_name> <repo_url>")
        print("  python3 DG_app_manager.py --suite <suite_name>")
        return

    # Determine location for managed apps (DG_vibecoder/apps_managed)
    root = Path(__file__).resolve().parent.parent
    apps_managed = root / "apps_managed"
    apps_managed.mkdir(exist_ok=True)
    
    manager = AppManager(apps_managed)

    arg1 = sys.argv[1]
    arg2 = sys.argv[2]

    if arg1 == "--suite":
        manager.install_suite(arg2)
    else:
        # Standard mode: app_name, repo_url
        manager.deploy_app(arg1, arg2)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: core.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
</CONTENT>
=== FILE END ===

=== FILE START: DG_dependency_scanner.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: DG_vibecoder_github_push.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

</CONTENT>
=== FILE END ===

=== SUMMARY ===
total_files=4

=== FILE END: modules/logs/20251211_213730/history/overseer_adjust.txt ===

=== FILE START: modules/logs/20251211_213730/overseer_adjust.txt ===
=== OVERSEER SNAPSHOT ===
generated=2025-12-11 21:37:30
target_folder=/home/seanf/workspace/deadlygraphics/ai/apps/DG_vibecoder/modules

=== FILES ===
=== FILE START: DG_app_manager.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_app_manager.py
Part of the Deadly Graphics Suite.

Purpose:
  Orchestrates the deployment of external AI applications (ComfyUI, OneTrainer, etc.).
  Implements the "Onion Model":
  - Layer 1: Git clone / Pull
  - Layer 2: UV-accelerated Virtual Environment
  - Layer 3: Dependency Installation
  - Layer 4: Integration (Scanner + Manifest)
"""

import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Import our scanner
try:
    from . import DG_dependency_scanner
except ImportError:
    import DG_dependency_scanner

# ============================================================
# CONFIGURATION: SUITES & PRESETS
# ============================================================

UV_INSTALL_SCRIPT = "curl -LsSf https://astral.sh/uv/install.sh | sh"

DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

# ============================================================
# APP MANAGER LOGIC
# ============================================================

class AppManager:
    def __init__(self, apps_root: Path):
        self.apps_root = apps_root
        self.uv_bin = self._find_uv()

    def _find_uv(self):
        """Locates 'uv' executable or returns None."""
        return shutil.which("uv")

    def ensure_uv(self):
        """Installs uv if missing."""
        if self.uv_bin:
            # print(f"[OK] uv detected: {self.uv_bin}")
            return
        
        print("[INSTALL] Installing uv (The Accelerator)...")
        try:
            subprocess.run(UV_INSTALL_SCRIPT, shell=True, check=True)
            print("[SUCCESS] uv installed.")
            self.uv_bin = shutil.which("uv") 
        except subprocess.CalledProcessError as e:
            print(f"[ERROR] Failed to install uv: {e}")
            raise

    def clone_repo(self, repo_url: str, app_name: str):
        """Clones a repository into apps_root/app_name, or pulls if exists."""
        target_dir = self.apps_root / app_name
        
        if target_dir.exists():
            print(f"[INFO] App directory exists: {target_dir}")
            print(f"[UPDATE] Attempting git pull...")
            # Non-blocking pull (check=False) as per architecture
            try:
                subprocess.run(["git", "pull"], cwd=target_dir, check=False)
            except Exception as e:
                print(f"[WARN] Git pull failed, continuing deployment: {e}")
            return target_dir
            
        print(f"[CLONE] Cloning {app_name} from {repo_url}...")
        subprocess.run(["git", "clone", repo_url, str(target_dir)], check=True)
        return target_dir

    def create_venv(self, app_dir: Path):
        """Creates a venv using uv."""
        venv_dir = app_dir / ".venv"
        if venv_dir.exists():
            return venv_dir

        print(f"[VENV] Creating venv for {app_dir.name} using uv...")
        subprocess.run(["uv", "venv", ".venv"], cwd=app_dir, check=True)
        return venv_dir

    def install_deps(self, app_dir: Path):
        """Installs dependencies from requirements.txt using uv pip."""
        req_file = app_dir / "requirements.txt"
        if not req_file.exists():
            print(f"[WARN] No requirements.txt found in {app_dir.name}")
            return

        print(f"[INSTALL] Installing dependencies for {app_dir.name}...")
        # uv auto-detects .venv in cwd
        subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=app_dir, check=True)

    def scan_app(self, app_dir: Path):
        """Runs the DG_dependency_scanner on the new app."""
        print(f"[SCAN] Generating manifest for {app_dir.name}...")
        data = DG_dependency_scanner.scan_directory(app_dir)
        
        manifest_path = app_dir / "manifest.json"
        with open(manifest_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2)
        print(f"[SUCCESS] Manifest saved.")

    def deploy_app(self, app_name: str, repo_url: str):
        """Full deployment pipeline for a single app."""
        print(f"\n=== DEPLOYING: {app_name} ===")
        self.ensure_uv()
        
        app_dir = self.clone_repo(repo_url, app_name)
        self.create_venv(app_dir)
        self.install_deps(app_dir)
        self.scan_app(app_dir)
        
        print(f"=== DEPLOYMENT COMPLETE: {app_name} ===\n")

    def install_suite(self, suite_name: str):
        """Orchestrates installation of a named suite."""
        if suite_name not in DG_SUITES:
            print(f"[ERROR] Suite '{suite_name}' not defined in DG_app_manager.")
            print(f"Available suites: {list(DG_SUITES.keys())}")
            return

        print(f"*******************************************")
        print(f"*** STARTING SUITE INSTALL: {suite_name}")
        print(f"*******************************************")
        
        apps = DG_SUITES[suite_name]
        for name, url in apps.items():
            try:
                self.deploy_app(name, url)
            except Exception as e:
                print(f"[CRITICAL] Failed to deploy {name}: {e}")
                print("Continuing to next app in suite...")
        
        print(f"*******************************************")
        print(f"*** SUITE INSTALL COMPLETE: {suite_name}")
        print(f"*******************************************")

# ============================================================
# CLI ENTRYPOINT
# ============================================================

def main():
    if len(sys.argv) < 3:
        print("Usage:")
        print("  python3 DG_app_manager.py <app_name> <repo_url>")
        print("  python3 DG_app_manager.py --suite <suite_name>")
        return

    # Determine location for managed apps (DG_vibecoder/apps_managed)
    root = Path(__file__).resolve().parent.parent
    apps_managed = root / "apps_managed"
    apps_managed.mkdir(exist_ok=True)
    
    manager = AppManager(apps_managed)

    arg1 = sys.argv[1]
    arg2 = sys.argv[2]

    if arg1 == "--suite":
        manager.install_suite(arg2)
    else:
        # Standard mode: app_name, repo_url
        manager.deploy_app(arg1, arg2)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: core.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
</CONTENT>
=== FILE END ===

=== FILE START: DG_dependency_scanner.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: DG_vibecoder_github_push.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

</CONTENT>
=== FILE END ===

=== SUMMARY ===
total_files=4

=== FILE END: modules/logs/20251211_213730/overseer_adjust.txt ===

=== FILE START: modules/logs/20251212_015144/history/overseer_adjust.txt ===
=== OVERSEER SNAPSHOT ===
generated=2025-12-12 01:51:44
target_folder=/home/seanf/workspace/deadlygraphics/ai/apps/DG_vibecoder/modules

=== FILES ===
=== FILE START: DG_app_manager.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Try import scanner
try: from . import DG_dependency_scanner
except ImportError: import DG_dependency_scanner

# SUITES
DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

class AppManager:
    def __init__(self, root):
        self.root = root
        self.uv = shutil.which("uv")

    def ensure_uv(self):
        if self.uv: return
        print("[INSTALL] Installing uv...")
        subprocess.run("curl -LsSf https://astral.sh/uv/install.sh | sh", shell=True, check=True)
        self.uv = shutil.which("uv")

    def deploy(self, name, url):
        print(f"\n=== DEPLOYING: {name} ===")
        self.ensure_uv()
        target = self.root / name
        
        # Clone or Pull
        if target.exists():
            print(f"[UPDATE] git pull...")
            subprocess.run(["git", "pull"], cwd=target, check=False)
        else:
            print(f"[CLONE] {url}")
            subprocess.run(["git", "clone", url, str(target)], check=True)

        # Venv & Deps
        print("[VENV] Creating venv...")
        subprocess.run(["uv", "venv", ".venv"], cwd=target, check=True)
        
        if (target / "requirements.txt").exists():
            print("[INSTALL] Deps...")
            subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)
        
        # Scan
        print("[SCAN] Generating manifest...")
        data = DG_dependency_scanner.scan_directory(target)
        with open(target / "manifest.json", "w") as f: json.dump(data, f, indent=2)
        print(f"[SUCCESS] {name} deployed.")

    def install_suite(self, suite):
        if suite not in DG_SUITES: return print(f"[ERROR] Unknown suite: {suite}")
        print(f"*** SUITE: {suite} ***")
        for name, url in DG_SUITES[suite].items():
            try: self.deploy(name, url)
            except Exception as e: print(f"[FAIL] {name}: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 3: sys.exit(1)
    root = Path(__file__).resolve().parent.parent / "apps_managed"
    root.mkdir(exist_ok=True)
    mgr = AppManager(root)
    
    if sys.argv[1] == "--suite": mgr.install_suite(sys.argv[2])
    else: mgr.deploy(sys.argv[1], sys.argv[2])

</CONTENT>
=== FILE END ===

=== FILE START: core.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
</CONTENT>
=== FILE END ===

=== FILE START: DG_dependency_scanner.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: DG_vibecoder_github_push.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

</CONTENT>
=== FILE END ===

=== SUMMARY ===
total_files=4

=== FILE END: modules/logs/20251212_015144/history/overseer_adjust.txt ===

=== FILE START: modules/logs/20251212_015144/overseer_adjust.txt ===
=== OVERSEER SNAPSHOT ===
generated=2025-12-12 01:51:44
target_folder=/home/seanf/workspace/deadlygraphics/ai/apps/DG_vibecoder/modules

=== FILES ===
=== FILE START: DG_app_manager.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Try import scanner
try: from . import DG_dependency_scanner
except ImportError: import DG_dependency_scanner

# SUITES
DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

class AppManager:
    def __init__(self, root):
        self.root = root
        self.uv = shutil.which("uv")

    def ensure_uv(self):
        if self.uv: return
        print("[INSTALL] Installing uv...")
        subprocess.run("curl -LsSf https://astral.sh/uv/install.sh | sh", shell=True, check=True)
        self.uv = shutil.which("uv")

    def deploy(self, name, url):
        print(f"\n=== DEPLOYING: {name} ===")
        self.ensure_uv()
        target = self.root / name
        
        # Clone or Pull
        if target.exists():
            print(f"[UPDATE] git pull...")
            subprocess.run(["git", "pull"], cwd=target, check=False)
        else:
            print(f"[CLONE] {url}")
            subprocess.run(["git", "clone", url, str(target)], check=True)

        # Venv & Deps
        print("[VENV] Creating venv...")
        subprocess.run(["uv", "venv", ".venv"], cwd=target, check=True)
        
        if (target / "requirements.txt").exists():
            print("[INSTALL] Deps...")
            subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)
        
        # Scan
        print("[SCAN] Generating manifest...")
        data = DG_dependency_scanner.scan_directory(target)
        with open(target / "manifest.json", "w") as f: json.dump(data, f, indent=2)
        print(f"[SUCCESS] {name} deployed.")

    def install_suite(self, suite):
        if suite not in DG_SUITES: return print(f"[ERROR] Unknown suite: {suite}")
        print(f"*** SUITE: {suite} ***")
        for name, url in DG_SUITES[suite].items():
            try: self.deploy(name, url)
            except Exception as e: print(f"[FAIL] {name}: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 3: sys.exit(1)
    root = Path(__file__).resolve().parent.parent / "apps_managed"
    root.mkdir(exist_ok=True)
    mgr = AppManager(root)
    
    if sys.argv[1] == "--suite": mgr.install_suite(sys.argv[2])
    else: mgr.deploy(sys.argv[1], sys.argv[2])

</CONTENT>
=== FILE END ===

=== FILE START: core.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
</CONTENT>
=== FILE END ===

=== FILE START: DG_dependency_scanner.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: DG_vibecoder_github_push.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

</CONTENT>
=== FILE END ===

=== SUMMARY ===
total_files=4

=== FILE END: modules/logs/20251212_015144/overseer_adjust.txt ===

=== FILE START: modules/logs/20251212_015149/history/overseer_adjust.txt ===
=== OVERSEER SNAPSHOT ===
generated=2025-12-12 01:51:49
target_folder=/home/seanf/workspace/deadlygraphics/ai/apps/DG_vibecoder/modules

=== FILES ===
=== FILE START: DG_app_manager.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Try import scanner
try: from . import DG_dependency_scanner
except ImportError: import DG_dependency_scanner

# SUITES
DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

class AppManager:
    def __init__(self, root):
        self.root = root
        self.uv = shutil.which("uv")

    def ensure_uv(self):
        if self.uv: return
        print("[INSTALL] Installing uv...")
        subprocess.run("curl -LsSf https://astral.sh/uv/install.sh | sh", shell=True, check=True)
        self.uv = shutil.which("uv")

    def deploy(self, name, url):
        print(f"\n=== DEPLOYING: {name} ===")
        self.ensure_uv()
        target = self.root / name
        
        # Clone or Pull
        if target.exists():
            print(f"[UPDATE] git pull...")
            subprocess.run(["git", "pull"], cwd=target, check=False)
        else:
            print(f"[CLONE] {url}")
            subprocess.run(["git", "clone", url, str(target)], check=True)

        # Venv & Deps
        print("[VENV] Creating venv...")
        subprocess.run(["uv", "venv", ".venv"], cwd=target, check=True)
        
        if (target / "requirements.txt").exists():
            print("[INSTALL] Deps...")
            subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)
        
        # Scan
        print("[SCAN] Generating manifest...")
        data = DG_dependency_scanner.scan_directory(target)
        with open(target / "manifest.json", "w") as f: json.dump(data, f, indent=2)
        print(f"[SUCCESS] {name} deployed.")

    def install_suite(self, suite):
        if suite not in DG_SUITES: return print(f"[ERROR] Unknown suite: {suite}")
        print(f"*** SUITE: {suite} ***")
        for name, url in DG_SUITES[suite].items():
            try: self.deploy(name, url)
            except Exception as e: print(f"[FAIL] {name}: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 3: sys.exit(1)
    root = Path(__file__).resolve().parent.parent / "apps_managed"
    root.mkdir(exist_ok=True)
    mgr = AppManager(root)
    
    if sys.argv[1] == "--suite": mgr.install_suite(sys.argv[2])
    else: mgr.deploy(sys.argv[1], sys.argv[2])

</CONTENT>
=== FILE END ===

=== FILE START: core.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
</CONTENT>
=== FILE END ===

=== FILE START: DG_dependency_scanner.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: DG_vibecoder_github_push.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

</CONTENT>
=== FILE END ===

=== SUMMARY ===
total_files=4

=== FILE END: modules/logs/20251212_015149/history/overseer_adjust.txt ===

=== FILE START: modules/logs/20251212_015149/overseer_adjust.txt ===
=== OVERSEER SNAPSHOT ===
generated=2025-12-12 01:51:49
target_folder=/home/seanf/workspace/deadlygraphics/ai/apps/DG_vibecoder/modules

=== FILES ===
=== FILE START: DG_app_manager.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Try import scanner
try: from . import DG_dependency_scanner
except ImportError: import DG_dependency_scanner

# SUITES
DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

class AppManager:
    def __init__(self, root):
        self.root = root
        self.uv = shutil.which("uv")

    def ensure_uv(self):
        if self.uv: return
        print("[INSTALL] Installing uv...")
        subprocess.run("curl -LsSf https://astral.sh/uv/install.sh | sh", shell=True, check=True)
        self.uv = shutil.which("uv")

    def deploy(self, name, url):
        print(f"\n=== DEPLOYING: {name} ===")
        self.ensure_uv()
        target = self.root / name
        
        # Clone or Pull
        if target.exists():
            print(f"[UPDATE] git pull...")
            subprocess.run(["git", "pull"], cwd=target, check=False)
        else:
            print(f"[CLONE] {url}")
            subprocess.run(["git", "clone", url, str(target)], check=True)

        # Venv & Deps
        print("[VENV] Creating venv...")
        subprocess.run(["uv", "venv", ".venv"], cwd=target, check=True)
        
        if (target / "requirements.txt").exists():
            print("[INSTALL] Deps...")
            subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)
        
        # Scan
        print("[SCAN] Generating manifest...")
        data = DG_dependency_scanner.scan_directory(target)
        with open(target / "manifest.json", "w") as f: json.dump(data, f, indent=2)
        print(f"[SUCCESS] {name} deployed.")

    def install_suite(self, suite):
        if suite not in DG_SUITES: return print(f"[ERROR] Unknown suite: {suite}")
        print(f"*** SUITE: {suite} ***")
        for name, url in DG_SUITES[suite].items():
            try: self.deploy(name, url)
            except Exception as e: print(f"[FAIL] {name}: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 3: sys.exit(1)
    root = Path(__file__).resolve().parent.parent / "apps_managed"
    root.mkdir(exist_ok=True)
    mgr = AppManager(root)
    
    if sys.argv[1] == "--suite": mgr.install_suite(sys.argv[2])
    else: mgr.deploy(sys.argv[1], sys.argv[2])

</CONTENT>
=== FILE END ===

=== FILE START: core.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
</CONTENT>
=== FILE END ===

=== FILE START: DG_dependency_scanner.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: DG_vibecoder_github_push.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

</CONTENT>
=== FILE END ===

=== SUMMARY ===
total_files=4

=== FILE END: modules/logs/20251212_015149/overseer_adjust.txt ===

=== FILE START: modules/logs/20251212_015335/history/overseer_adjust.txt ===
=== OVERSEER SNAPSHOT ===
generated=2025-12-12 01:53:35
target_folder=/home/seanf/workspace/deadlygraphics/ai/apps/DG_vibecoder/modules

=== FILES ===
=== FILE START: DG_app_manager.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Try import scanner
try: from . import DG_dependency_scanner
except ImportError: import DG_dependency_scanner

# SUITES
DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

class AppManager:
    def __init__(self, root):
        self.root = root
        self.uv = shutil.which("uv")

    def ensure_uv(self):
        if self.uv: return
        print("[INSTALL] Installing uv...")
        subprocess.run("curl -LsSf https://astral.sh/uv/install.sh | sh", shell=True, check=True)
        self.uv = shutil.which("uv")

    def deploy(self, name, url):
        print(f"\n=== DEPLOYING: {name} ===")
        self.ensure_uv()
        target = self.root / name
        
        # Clone or Pull
        if target.exists():
            print(f"[UPDATE] git pull...")
            subprocess.run(["git", "pull"], cwd=target, check=False)
        else:
            print(f"[CLONE] {url}")
            subprocess.run(["git", "clone", url, str(target)], check=True)

        # Venv & Deps
        print("[VENV] Creating venv...")
        subprocess.run(["uv", "venv", ".venv"], cwd=target, check=True)
        
        if (target / "requirements.txt").exists():
            print("[INSTALL] Deps...")
            subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)
        
        # Scan
        print("[SCAN] Generating manifest...")
        data = DG_dependency_scanner.scan_directory(target)
        with open(target / "manifest.json", "w") as f: json.dump(data, f, indent=2)
        print(f"[SUCCESS] {name} deployed.")

    def install_suite(self, suite):
        if suite not in DG_SUITES: return print(f"[ERROR] Unknown suite: {suite}")
        print(f"*** SUITE: {suite} ***")
        for name, url in DG_SUITES[suite].items():
            try: self.deploy(name, url)
            except Exception as e: print(f"[FAIL] {name}: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 3: sys.exit(1)
    root = Path(__file__).resolve().parent.parent / "apps_managed"
    root.mkdir(exist_ok=True)
    mgr = AppManager(root)
    
    if sys.argv[1] == "--suite": mgr.install_suite(sys.argv[2])
    else: mgr.deploy(sys.argv[1], sys.argv[2])

</CONTENT>
=== FILE END ===

=== FILE START: core.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
</CONTENT>
=== FILE END ===

=== FILE START: DG_dependency_scanner.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: DG_vibecoder_github_push.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

</CONTENT>
=== FILE END ===

=== SUMMARY ===
total_files=4

=== FILE END: modules/logs/20251212_015335/history/overseer_adjust.txt ===

=== FILE START: modules/logs/20251212_015335/overseer_adjust.txt ===
=== OVERSEER SNAPSHOT ===
generated=2025-12-12 01:53:35
target_folder=/home/seanf/workspace/deadlygraphics/ai/apps/DG_vibecoder/modules

=== FILES ===
=== FILE START: DG_app_manager.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Try import scanner
try: from . import DG_dependency_scanner
except ImportError: import DG_dependency_scanner

# SUITES
DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

class AppManager:
    def __init__(self, root):
        self.root = root
        self.uv = shutil.which("uv")

    def ensure_uv(self):
        if self.uv: return
        print("[INSTALL] Installing uv...")
        subprocess.run("curl -LsSf https://astral.sh/uv/install.sh | sh", shell=True, check=True)
        self.uv = shutil.which("uv")

    def deploy(self, name, url):
        print(f"\n=== DEPLOYING: {name} ===")
        self.ensure_uv()
        target = self.root / name
        
        # Clone or Pull
        if target.exists():
            print(f"[UPDATE] git pull...")
            subprocess.run(["git", "pull"], cwd=target, check=False)
        else:
            print(f"[CLONE] {url}")
            subprocess.run(["git", "clone", url, str(target)], check=True)

        # Venv & Deps
        print("[VENV] Creating venv...")
        subprocess.run(["uv", "venv", ".venv"], cwd=target, check=True)
        
        if (target / "requirements.txt").exists():
            print("[INSTALL] Deps...")
            subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)
        
        # Scan
        print("[SCAN] Generating manifest...")
        data = DG_dependency_scanner.scan_directory(target)
        with open(target / "manifest.json", "w") as f: json.dump(data, f, indent=2)
        print(f"[SUCCESS] {name} deployed.")

    def install_suite(self, suite):
        if suite not in DG_SUITES: return print(f"[ERROR] Unknown suite: {suite}")
        print(f"*** SUITE: {suite} ***")
        for name, url in DG_SUITES[suite].items():
            try: self.deploy(name, url)
            except Exception as e: print(f"[FAIL] {name}: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 3: sys.exit(1)
    root = Path(__file__).resolve().parent.parent / "apps_managed"
    root.mkdir(exist_ok=True)
    mgr = AppManager(root)
    
    if sys.argv[1] == "--suite": mgr.install_suite(sys.argv[2])
    else: mgr.deploy(sys.argv[1], sys.argv[2])

</CONTENT>
=== FILE END ===

=== FILE START: core.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
</CONTENT>
=== FILE END ===

=== FILE START: DG_dependency_scanner.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: DG_vibecoder_github_push.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

</CONTENT>
=== FILE END ===

=== SUMMARY ===
total_files=4

=== FILE END: modules/logs/20251212_015335/overseer_adjust.txt ===

=== FILE START: modules/logs/20251212_015512/history/overseer_adjust.txt ===
=== OVERSEER SNAPSHOT ===
generated=2025-12-12 01:55:12
target_folder=/home/seanf/workspace/deadlygraphics/ai/apps/DG_vibecoder/modules

=== FILES ===
=== FILE START: DG_app_manager.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Try import scanner
try: from . import DG_dependency_scanner
except ImportError: import DG_dependency_scanner

# SUITES
DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

class AppManager:
    def __init__(self, root):
        self.root = root
        self.uv = shutil.which("uv")

    def ensure_uv(self):
        if self.uv: return
        print("[INSTALL] Installing uv...")
        subprocess.run("curl -LsSf https://astral.sh/uv/install.sh | sh", shell=True, check=True)
        self.uv = shutil.which("uv")

    def deploy(self, name, url):
        print(f"\n=== DEPLOYING: {name} ===")
        self.ensure_uv()
        target = self.root / name
        
        # Clone or Pull
        if target.exists():
            print(f"[UPDATE] git pull...")
            subprocess.run(["git", "pull"], cwd=target, check=False)
        else:
            print(f"[CLONE] {url}")
            subprocess.run(["git", "clone", url, str(target)], check=True)

        # Venv & Deps
        print("[VENV] Creating venv...")
        subprocess.run(["uv", "venv", ".venv"], cwd=target, check=True)
        
        if (target / "requirements.txt").exists():
            print("[INSTALL] Deps...")
            subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)
        
        # Scan
        print("[SCAN] Generating manifest...")
        data = DG_dependency_scanner.scan_directory(target)
        with open(target / "manifest.json", "w") as f: json.dump(data, f, indent=2)
        print(f"[SUCCESS] {name} deployed.")

    def install_suite(self, suite):
        if suite not in DG_SUITES: return print(f"[ERROR] Unknown suite: {suite}")
        print(f"*** SUITE: {suite} ***")
        for name, url in DG_SUITES[suite].items():
            try: self.deploy(name, url)
            except Exception as e: print(f"[FAIL] {name}: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 3: sys.exit(1)
    root = Path(__file__).resolve().parent.parent / "apps_managed"
    root.mkdir(exist_ok=True)
    mgr = AppManager(root)
    
    if sys.argv[1] == "--suite": mgr.install_suite(sys.argv[2])
    else: mgr.deploy(sys.argv[1], sys.argv[2])

</CONTENT>
=== FILE END ===

=== FILE START: core.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
</CONTENT>
=== FILE END ===

=== FILE START: DG_dependency_scanner.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: DG_vibecoder_github_push.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

</CONTENT>
=== FILE END ===

=== SUMMARY ===
total_files=4

=== FILE END: modules/logs/20251212_015512/history/overseer_adjust.txt ===

=== FILE START: modules/logs/20251212_015512/overseer_adjust.txt ===
=== OVERSEER SNAPSHOT ===
generated=2025-12-12 01:55:12
target_folder=/home/seanf/workspace/deadlygraphics/ai/apps/DG_vibecoder/modules

=== FILES ===
=== FILE START: DG_app_manager.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Try import scanner
try: from . import DG_dependency_scanner
except ImportError: import DG_dependency_scanner

# SUITES
DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

class AppManager:
    def __init__(self, root):
        self.root = root
        self.uv = shutil.which("uv")

    def ensure_uv(self):
        if self.uv: return
        print("[INSTALL] Installing uv...")
        subprocess.run("curl -LsSf https://astral.sh/uv/install.sh | sh", shell=True, check=True)
        self.uv = shutil.which("uv")

    def deploy(self, name, url):
        print(f"\n=== DEPLOYING: {name} ===")
        self.ensure_uv()
        target = self.root / name
        
        # Clone or Pull
        if target.exists():
            print(f"[UPDATE] git pull...")
            subprocess.run(["git", "pull"], cwd=target, check=False)
        else:
            print(f"[CLONE] {url}")
            subprocess.run(["git", "clone", url, str(target)], check=True)

        # Venv & Deps
        print("[VENV] Creating venv...")
        subprocess.run(["uv", "venv", ".venv"], cwd=target, check=True)
        
        if (target / "requirements.txt").exists():
            print("[INSTALL] Deps...")
            subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)
        
        # Scan
        print("[SCAN] Generating manifest...")
        data = DG_dependency_scanner.scan_directory(target)
        with open(target / "manifest.json", "w") as f: json.dump(data, f, indent=2)
        print(f"[SUCCESS] {name} deployed.")

    def install_suite(self, suite):
        if suite not in DG_SUITES: return print(f"[ERROR] Unknown suite: {suite}")
        print(f"*** SUITE: {suite} ***")
        for name, url in DG_SUITES[suite].items():
            try: self.deploy(name, url)
            except Exception as e: print(f"[FAIL] {name}: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 3: sys.exit(1)
    root = Path(__file__).resolve().parent.parent / "apps_managed"
    root.mkdir(exist_ok=True)
    mgr = AppManager(root)
    
    if sys.argv[1] == "--suite": mgr.install_suite(sys.argv[2])
    else: mgr.deploy(sys.argv[1], sys.argv[2])

</CONTENT>
=== FILE END ===

=== FILE START: core.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
</CONTENT>
=== FILE END ===

=== FILE START: DG_dependency_scanner.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: DG_vibecoder_github_push.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

</CONTENT>
=== FILE END ===

=== SUMMARY ===
total_files=4

=== FILE END: modules/logs/20251212_015512/overseer_adjust.txt ===

=== FILE START: modules/logs/20251212_015913/history/overseer_adjust.txt ===
=== OVERSEER SNAPSHOT ===
generated=2025-12-12 01:59:13
target_folder=/home/seanf/workspace/deadlygraphics/ai/apps/DG_vibecoder/modules

=== FILES ===
=== FILE START: DG_app_manager.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Try import scanner
try: from . import DG_dependency_scanner
except ImportError: import DG_dependency_scanner

# SUITES
DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

class AppManager:
    def __init__(self, root):
        self.apps_root = root
        self.uv = shutil.which("uv")

    def ensure_uv(self):
        if self.uv: return
        print("[INSTALL] Installing uv...")
        subprocess.run("curl -LsSf https://astral.sh/uv/install.sh | sh", shell=True, check=True)
        self.uv = shutil.which("uv")

    def clone_repo(self, repo_url, app_name):
        target = self.apps_root / app_name
        if target.exists():
            print(f"[UPDATE] git pull for {app_name}...")
            subprocess.run(["git", "pull"], cwd=target, check=False)
        else:
            print(f"[CLONE] {app_name}...")
            subprocess.run(["git", "clone", repo_url, str(target)], check=True)
        
        # FIX: Kohya requires submodules
        if app_name == "Kohya_ss":
            print("[SUBMODULE] Init for Kohya...")
            subprocess.run(["git", "submodule", "update", "--init", "--recursive"], cwd=target, check=False)
        return target

    def install_deps(self, target):
        print(f"[INSTALL] Dependencies for {target.name}...")
        
        # Strategy 1: Fast (UV)
        try:
            subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)
        except subprocess.CalledProcessError:
            print(f"[WARN] 'uv pip' failed. Attempting standard pip fallback...")
            # Strategy 2: Compat (Standard Pip via UV venv)
            subprocess.run(["uv", "pip", "install", "pip"], cwd=target, check=True)
            subprocess.run(["uv", "run", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)

    def scan_app(self, target):
        print("[SCAN] Generating manifest...")
        try:
            data = DG_dependency_scanner.scan_directory(target)
            with open(target / "manifest.json", "w") as f: json.dump(data, f, indent=2)
            print("[SUCCESS] Manifest saved.")
        except Exception as e:
            print(f"[WARN] Scanner skipped: {e}")

    def deploy(self, name, url):
        print(f"\n=== DEPLOYING: {name} ===")
        self.ensure_uv()
        target = self.clone_repo(url, name)
        
        print(f"[VENV] Creating venv for {name}...")
        subprocess.run(["uv", "venv", ".venv"], cwd=target, check=True)
        
        if (target / "requirements.txt").exists():
            self.install_deps(target)
        
        self.scan_app(target)
        print(f"[SUCCESS] {name} deployed.")

    def install_suite(self, suite):
        if suite not in DG_SUITES: return print(f"[ERROR] Unknown suite: {suite}")
        print(f"*** SUITE: {suite} ***")
        for name, url in DG_SUITES[suite].items():
            try: self.deploy(name, url)
            except Exception as e: print(f"[FAIL] {name}: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 3: sys.exit(1)
    # Fix path logic to ensure apps_managed is in correct spot
    root = Path(__file__).resolve().parent.parent / "apps_managed"
    root.mkdir(exist_ok=True)
    mgr = AppManager(root)
    
    if sys.argv[1] == "--suite": mgr.install_suite(sys.argv[2])
    else: mgr.deploy(sys.argv[1], sys.argv[2])

</CONTENT>
=== FILE END ===

=== FILE START: core.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
</CONTENT>
=== FILE END ===

=== FILE START: DG_dependency_scanner.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: DG_vibecoder_github_push.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

</CONTENT>
=== FILE END ===

=== SUMMARY ===
total_files=4

=== FILE END: modules/logs/20251212_015913/history/overseer_adjust.txt ===

=== FILE START: modules/logs/20251212_015913/overseer_adjust.txt ===
=== OVERSEER SNAPSHOT ===
generated=2025-12-12 01:59:13
target_folder=/home/seanf/workspace/deadlygraphics/ai/apps/DG_vibecoder/modules

=== FILES ===
=== FILE START: DG_app_manager.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Try import scanner
try: from . import DG_dependency_scanner
except ImportError: import DG_dependency_scanner

# SUITES
DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

class AppManager:
    def __init__(self, root):
        self.apps_root = root
        self.uv = shutil.which("uv")

    def ensure_uv(self):
        if self.uv: return
        print("[INSTALL] Installing uv...")
        subprocess.run("curl -LsSf https://astral.sh/uv/install.sh | sh", shell=True, check=True)
        self.uv = shutil.which("uv")

    def clone_repo(self, repo_url, app_name):
        target = self.apps_root / app_name
        if target.exists():
            print(f"[UPDATE] git pull for {app_name}...")
            subprocess.run(["git", "pull"], cwd=target, check=False)
        else:
            print(f"[CLONE] {app_name}...")
            subprocess.run(["git", "clone", repo_url, str(target)], check=True)
        
        # FIX: Kohya requires submodules
        if app_name == "Kohya_ss":
            print("[SUBMODULE] Init for Kohya...")
            subprocess.run(["git", "submodule", "update", "--init", "--recursive"], cwd=target, check=False)
        return target

    def install_deps(self, target):
        print(f"[INSTALL] Dependencies for {target.name}...")
        
        # Strategy 1: Fast (UV)
        try:
            subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)
        except subprocess.CalledProcessError:
            print(f"[WARN] 'uv pip' failed. Attempting standard pip fallback...")
            # Strategy 2: Compat (Standard Pip via UV venv)
            subprocess.run(["uv", "pip", "install", "pip"], cwd=target, check=True)
            subprocess.run(["uv", "run", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)

    def scan_app(self, target):
        print("[SCAN] Generating manifest...")
        try:
            data = DG_dependency_scanner.scan_directory(target)
            with open(target / "manifest.json", "w") as f: json.dump(data, f, indent=2)
            print("[SUCCESS] Manifest saved.")
        except Exception as e:
            print(f"[WARN] Scanner skipped: {e}")

    def deploy(self, name, url):
        print(f"\n=== DEPLOYING: {name} ===")
        self.ensure_uv()
        target = self.clone_repo(url, name)
        
        print(f"[VENV] Creating venv for {name}...")
        subprocess.run(["uv", "venv", ".venv"], cwd=target, check=True)
        
        if (target / "requirements.txt").exists():
            self.install_deps(target)
        
        self.scan_app(target)
        print(f"[SUCCESS] {name} deployed.")

    def install_suite(self, suite):
        if suite not in DG_SUITES: return print(f"[ERROR] Unknown suite: {suite}")
        print(f"*** SUITE: {suite} ***")
        for name, url in DG_SUITES[suite].items():
            try: self.deploy(name, url)
            except Exception as e: print(f"[FAIL] {name}: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 3: sys.exit(1)
    # Fix path logic to ensure apps_managed is in correct spot
    root = Path(__file__).resolve().parent.parent / "apps_managed"
    root.mkdir(exist_ok=True)
    mgr = AppManager(root)
    
    if sys.argv[1] == "--suite": mgr.install_suite(sys.argv[2])
    else: mgr.deploy(sys.argv[1], sys.argv[2])

</CONTENT>
=== FILE END ===

=== FILE START: core.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
</CONTENT>
=== FILE END ===

=== FILE START: DG_dependency_scanner.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: DG_vibecoder_github_push.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

</CONTENT>
=== FILE END ===

=== SUMMARY ===
total_files=4

=== FILE END: modules/logs/20251212_015913/overseer_adjust.txt ===

=== FILE START: modules/logs/20251212_020014/history/overseer_adjust.txt ===
=== OVERSEER SNAPSHOT ===
generated=2025-12-12 02:00:14
target_folder=/home/seanf/workspace/deadlygraphics/ai/apps/DG_vibecoder/modules

=== FILES ===
=== FILE START: DG_app_manager.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Try import scanner
try: from . import DG_dependency_scanner
except ImportError: import DG_dependency_scanner

# SUITES
DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

class AppManager:
    def __init__(self, root):
        self.apps_root = root
        self.uv = shutil.which("uv")

    def ensure_uv(self):
        if self.uv: return
        print("[INSTALL] Installing uv...")
        subprocess.run("curl -LsSf https://astral.sh/uv/install.sh | sh", shell=True, check=True)
        self.uv = shutil.which("uv")

    def clone_repo(self, repo_url, app_name):
        target = self.apps_root / app_name
        if target.exists():
            print(f"[UPDATE] git pull for {app_name}...")
            subprocess.run(["git", "pull"], cwd=target, check=False)
        else:
            print(f"[CLONE] {app_name}...")
            subprocess.run(["git", "clone", repo_url, str(target)], check=True)
        
        # FIX: Kohya requires submodules
        if app_name == "Kohya_ss":
            print("[SUBMODULE] Init for Kohya...")
            subprocess.run(["git", "submodule", "update", "--init", "--recursive"], cwd=target, check=False)
        return target

    def install_deps(self, target):
        print(f"[INSTALL] Dependencies for {target.name}...")
        
        # Strategy 1: Fast (UV)
        try:
            subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)
        except subprocess.CalledProcessError:
            print(f"[WARN] 'uv pip' failed. Attempting standard pip fallback...")
            # Strategy 2: Compat (Standard Pip via UV venv)
            subprocess.run(["uv", "pip", "install", "pip"], cwd=target, check=True)
            subprocess.run(["uv", "run", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)

    def scan_app(self, target):
        print("[SCAN] Generating manifest...")
        try:
            data = DG_dependency_scanner.scan_directory(target)
            with open(target / "manifest.json", "w") as f: json.dump(data, f, indent=2)
            print("[SUCCESS] Manifest saved.")
        except Exception as e:
            print(f"[WARN] Scanner skipped: {e}")

    def deploy(self, name, url):
        print(f"\n=== DEPLOYING: {name} ===")
        self.ensure_uv()
        target = self.clone_repo(url, name)
        
        print(f"[VENV] Creating venv for {name}...")
        subprocess.run(["uv", "venv", ".venv"], cwd=target, check=True)
        
        if (target / "requirements.txt").exists():
            self.install_deps(target)
        
        self.scan_app(target)
        print(f"[SUCCESS] {name} deployed.")

    def install_suite(self, suite):
        if suite not in DG_SUITES: return print(f"[ERROR] Unknown suite: {suite}")
        print(f"*** SUITE: {suite} ***")
        for name, url in DG_SUITES[suite].items():
            try: self.deploy(name, url)
            except Exception as e: print(f"[FAIL] {name}: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 3: sys.exit(1)
    # Fix path logic to ensure apps_managed is in correct spot
    root = Path(__file__).resolve().parent.parent / "apps_managed"
    root.mkdir(exist_ok=True)
    mgr = AppManager(root)
    
    if sys.argv[1] == "--suite": mgr.install_suite(sys.argv[2])
    else: mgr.deploy(sys.argv[1], sys.argv[2])

</CONTENT>
=== FILE END ===

=== FILE START: core.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
</CONTENT>
=== FILE END ===

=== FILE START: DG_dependency_scanner.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: DG_vibecoder_github_push.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

</CONTENT>
=== FILE END ===

=== SUMMARY ===
total_files=4

=== FILE END: modules/logs/20251212_020014/history/overseer_adjust.txt ===

=== FILE START: modules/logs/20251212_020014/overseer_adjust.txt ===
=== OVERSEER SNAPSHOT ===
generated=2025-12-12 02:00:14
target_folder=/home/seanf/workspace/deadlygraphics/ai/apps/DG_vibecoder/modules

=== FILES ===
=== FILE START: DG_app_manager.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Try import scanner
try: from . import DG_dependency_scanner
except ImportError: import DG_dependency_scanner

# SUITES
DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

class AppManager:
    def __init__(self, root):
        self.apps_root = root
        self.uv = shutil.which("uv")

    def ensure_uv(self):
        if self.uv: return
        print("[INSTALL] Installing uv...")
        subprocess.run("curl -LsSf https://astral.sh/uv/install.sh | sh", shell=True, check=True)
        self.uv = shutil.which("uv")

    def clone_repo(self, repo_url, app_name):
        target = self.apps_root / app_name
        if target.exists():
            print(f"[UPDATE] git pull for {app_name}...")
            subprocess.run(["git", "pull"], cwd=target, check=False)
        else:
            print(f"[CLONE] {app_name}...")
            subprocess.run(["git", "clone", repo_url, str(target)], check=True)
        
        # FIX: Kohya requires submodules
        if app_name == "Kohya_ss":
            print("[SUBMODULE] Init for Kohya...")
            subprocess.run(["git", "submodule", "update", "--init", "--recursive"], cwd=target, check=False)
        return target

    def install_deps(self, target):
        print(f"[INSTALL] Dependencies for {target.name}...")
        
        # Strategy 1: Fast (UV)
        try:
            subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)
        except subprocess.CalledProcessError:
            print(f"[WARN] 'uv pip' failed. Attempting standard pip fallback...")
            # Strategy 2: Compat (Standard Pip via UV venv)
            subprocess.run(["uv", "pip", "install", "pip"], cwd=target, check=True)
            subprocess.run(["uv", "run", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)

    def scan_app(self, target):
        print("[SCAN] Generating manifest...")
        try:
            data = DG_dependency_scanner.scan_directory(target)
            with open(target / "manifest.json", "w") as f: json.dump(data, f, indent=2)
            print("[SUCCESS] Manifest saved.")
        except Exception as e:
            print(f"[WARN] Scanner skipped: {e}")

    def deploy(self, name, url):
        print(f"\n=== DEPLOYING: {name} ===")
        self.ensure_uv()
        target = self.clone_repo(url, name)
        
        print(f"[VENV] Creating venv for {name}...")
        subprocess.run(["uv", "venv", ".venv"], cwd=target, check=True)
        
        if (target / "requirements.txt").exists():
            self.install_deps(target)
        
        self.scan_app(target)
        print(f"[SUCCESS] {name} deployed.")

    def install_suite(self, suite):
        if suite not in DG_SUITES: return print(f"[ERROR] Unknown suite: {suite}")
        print(f"*** SUITE: {suite} ***")
        for name, url in DG_SUITES[suite].items():
            try: self.deploy(name, url)
            except Exception as e: print(f"[FAIL] {name}: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 3: sys.exit(1)
    # Fix path logic to ensure apps_managed is in correct spot
    root = Path(__file__).resolve().parent.parent / "apps_managed"
    root.mkdir(exist_ok=True)
    mgr = AppManager(root)
    
    if sys.argv[1] == "--suite": mgr.install_suite(sys.argv[2])
    else: mgr.deploy(sys.argv[1], sys.argv[2])

</CONTENT>
=== FILE END ===

=== FILE START: core.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
</CONTENT>
=== FILE END ===

=== FILE START: DG_dependency_scanner.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: DG_vibecoder_github_push.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

</CONTENT>
=== FILE END ===

=== SUMMARY ===
total_files=4

=== FILE END: modules/logs/20251212_020014/overseer_adjust.txt ===

=== FILE START: modules/logs/overseer_response.txt ===
=== OVERSEER SNAPSHOT ===
generated=2025-12-12 02:00:14
target_folder=/home/seanf/workspace/deadlygraphics/ai/apps/DG_vibecoder/modules

=== FILES ===
=== FILE START: DG_app_manager.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import subprocess
import shutil
import json
from pathlib import Path

# Try import scanner
try: from . import DG_dependency_scanner
except ImportError: import DG_dependency_scanner

# SUITES
DG_SUITES = {
    "DG_AI": {
        "ComfyUI": "https://github.com/comfyanonymous/ComfyUI",
        "OneTrainer": "https://github.com/Nerogar/OneTrainer",
        "Kohya_ss": "https://github.com/bmaltais/kohya_ss",
        "AI-Toolkit": "https://github.com/ostris/ai-toolkit"
    }
}

class AppManager:
    def __init__(self, root):
        self.apps_root = root
        self.uv = shutil.which("uv")

    def ensure_uv(self):
        if self.uv: return
        print("[INSTALL] Installing uv...")
        subprocess.run("curl -LsSf https://astral.sh/uv/install.sh | sh", shell=True, check=True)
        self.uv = shutil.which("uv")

    def clone_repo(self, repo_url, app_name):
        target = self.apps_root / app_name
        if target.exists():
            print(f"[UPDATE] git pull for {app_name}...")
            subprocess.run(["git", "pull"], cwd=target, check=False)
        else:
            print(f"[CLONE] {app_name}...")
            subprocess.run(["git", "clone", repo_url, str(target)], check=True)
        
        # FIX: Kohya requires submodules
        if app_name == "Kohya_ss":
            print("[SUBMODULE] Init for Kohya...")
            subprocess.run(["git", "submodule", "update", "--init", "--recursive"], cwd=target, check=False)
        return target

    def install_deps(self, target):
        print(f"[INSTALL] Dependencies for {target.name}...")
        
        # Strategy 1: Fast (UV)
        try:
            subprocess.run(["uv", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)
        except subprocess.CalledProcessError:
            print(f"[WARN] 'uv pip' failed. Attempting standard pip fallback...")
            # Strategy 2: Compat (Standard Pip via UV venv)
            subprocess.run(["uv", "pip", "install", "pip"], cwd=target, check=True)
            subprocess.run(["uv", "run", "pip", "install", "-r", "requirements.txt"], cwd=target, check=True)

    def scan_app(self, target):
        print("[SCAN] Generating manifest...")
        try:
            data = DG_dependency_scanner.scan_directory(target)
            with open(target / "manifest.json", "w") as f: json.dump(data, f, indent=2)
            print("[SUCCESS] Manifest saved.")
        except Exception as e:
            print(f"[WARN] Scanner skipped: {e}")

    def deploy(self, name, url):
        print(f"\n=== DEPLOYING: {name} ===")
        self.ensure_uv()
        target = self.clone_repo(url, name)
        
        print(f"[VENV] Creating venv for {name}...")
        subprocess.run(["uv", "venv", ".venv"], cwd=target, check=True)
        
        if (target / "requirements.txt").exists():
            self.install_deps(target)
        
        self.scan_app(target)
        print(f"[SUCCESS] {name} deployed.")

    def install_suite(self, suite):
        if suite not in DG_SUITES: return print(f"[ERROR] Unknown suite: {suite}")
        print(f"*** SUITE: {suite} ***")
        for name, url in DG_SUITES[suite].items():
            try: self.deploy(name, url)
            except Exception as e: print(f"[FAIL] {name}: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 3: sys.exit(1)
    # Fix path logic to ensure apps_managed is in correct spot
    root = Path(__file__).resolve().parent.parent / "apps_managed"
    root.mkdir(exist_ok=True)
    mgr = AppManager(root)
    
    if sys.argv[1] == "--suite": mgr.install_suite(sys.argv[2])
    else: mgr.deploy(sys.argv[1], sys.argv[2])

</CONTENT>
=== FILE END ===

=== FILE START: core.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG Core Utilities
"""
import subprocess

# === VIBECORE: COMMAND SUMMARY START ===
def generate_summary_of_changes():
    """
    Reads the git diff and prints a friendly summary for console display.
    """
    try:
        diff = subprocess.check_output(["git", "diff", "--stat"], text=True)
        return diff.strip()
    except Exception as e:
        return f"[ERROR] Unable to produce summary: {e}"
# === VIBECORE: COMMAND SUMMARY END ===
</CONTENT>
=== FILE END ===

=== FILE START: DG_dependency_scanner.py ===
<CONTENT>
#!/usr/bin/env python3
"""
DG_dependency_scanner.py
Part of the Deadly Graphics Suite.

Purpose:
  Crawls a directory to identify:
  1. Python imports (via AST parsing)
  2. Package files (requirements.txt, pyproject.toml)
  3. GPU requirements (torch, tensorflow)
  4. System-level binary guesses (based on imports like cv2, soundfile)

Output:
  Generates a manifest.json in the target directory.
"""

import os
import sys
import json
import ast
import platform
from pathlib import Path

# ============================================================
# CONFIG / MAPPINGS
# ============================================================

# Map imports to Debian/Ubuntu system packages (heuristic)
SYS_PACKAGE_MAP = {
    "cv2": "libgl1-mesa-glx",
    "soundfile": "libsndfile1",
    "tk": "python3-tk",
    "PIL": "libjpeg-dev zlib1g-dev",  # Pillow deps
}

# Map imports to PyPI packages if names differ significantly
PYPI_MAP = {
    "cv2": "opencv-python",
    "PIL": "Pillow",
    "sklearn": "scikit-learn",
    "yaml": "PyYAML",
    "bs4": "beautifulsoup4",
}

CRITICAL_GPU_MODULES = {"torch", "tensorflow", "jax", "diffusers"}

# ============================================================
# SCANNERS
# ============================================================

def get_imports_from_file(filepath):
    """
    Parses a .py file using AST to extract all imported module names.
    Returns a set of top-level module names.
    """
    imports = set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=filepath.name)
        
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.add(alias.name.split('.')[0])
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.add(node.module.split('.')[0])
    except Exception as e:
        # verifying syntax errors or decoding issues don't crash the scanner
        print(f"[WARN] AST parse failed for {filepath.name}: {e}")
    
    return imports

def scan_directory(target_dir: Path):
    """
    Crawls the target directory for dependency clues.
    """
    scan_data = {
        "app_name": target_dir.name,
        "scan_time": None,  # Filled later
        "python_version_detected": platform.python_version(),
        "detected_imports": [],
        "config_files": [],
        "gpu_required": False,
        "suggested_system_packages": [],
        "suggested_pypi_packages": []
    }

    all_imports = set()
    has_requirements_txt = False
    has_pyproject = False

    # 1. Walk and Parse
    for root, _, files in os.walk(target_dir):
        if ".venv" in root or "__pycache__" in root or ".git" in root:
            continue
            
        for file in files:
            fpath = Path(root) / file
            
            # Check for config files
            if file == "requirements.txt":
                scan_data["config_files"].append("requirements.txt")
                has_requirements_txt = True
            elif file == "pyproject.toml":
                scan_data["config_files"].append("pyproject.toml")
                has_pyproject = True
            elif file == "environment.yml":
                scan_data["config_files"].append("environment.yml")

            # Parse Code
            if file.endswith(".py"):
                file_imports = get_imports_from_file(fpath)
                all_imports.update(file_imports)

    # 2. Filter Standard Library (Best Effort)
    # We compare against sys.stdlib_module_names if available (Py3.10+)
    if hasattr(sys, 'stdlib_module_names'):
        stdlib = sys.stdlib_module_names
        all_imports = {i for i in all_imports if i not in stdlib}
    
    # 3. Analyze Imports
    scan_data["detected_imports"] = sorted(list(all_imports))
    
    # Check GPU
    if not all_imports.isdisjoint(CRITICAL_GPU_MODULES):
        scan_data["gpu_required"] = True

    # Map to System Packages
    for mod in all_imports:
        if mod in SYS_PACKAGE_MAP:
            pkg = SYS_PACKAGE_MAP[mod]
            if pkg not in scan_data["suggested_system_packages"]:
                scan_data["suggested_system_packages"].append(pkg)

    # Map to PyPI (Simple mapping + identity)
    # This is a baseline; 'uv' or 'pip' will solve the rest.
    pypi_list = set()
    for mod in all_imports:
        mapped = PYPI_MAP.get(mod, mod)
        pypi_list.add(mapped)
    
    scan_data["suggested_pypi_packages"] = sorted(list(pypi_list))

    return scan_data

# ============================================================
# MAIN
# ============================================================

def main():
    if len(sys.argv) < 2:
        target_path = Path.cwd()
    else:
        target_path = Path(sys.argv[1]).resolve()

    if not target_path.exists():
        print(f"[ERROR] Path not found: {target_path}")
        sys.exit(1)

    print(f"[SCAN] Scanning: {target_path}")
    data = scan_directory(target_path)
    
    # Add timestamp
    from datetime import datetime
    data["scan_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Output Manifest
    manifest_path = target_path / "manifest.json"
    with open(manifest_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2)
    
    print(f"[SUCCESS] Manifest generated: {manifest_path}")
    
    # Preview
    print("-" * 40)
    print(f" App: {data['app_name']}")
    print(f" GPU Required: {data['gpu_required']}")
    print(f" Imports Found: {len(data['detected_imports'])}")
    print("-" * 40)

if __name__ == "__main__":
    main()
</CONTENT>
=== FILE END ===

=== FILE START: DG_vibecoder_github_push.py ===
<CONTENT>
#!/usr/bin/env python3
import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime

# ============================================================
# CONFIG
# ============================================================

CREDENTIALS_FILE = "/mnt/c/credentials/credentials.json"
INCLUDE_EXT = {".py", ".txt", ".md", ".toml", ".bat", ".sh"}
EXCLUDE_DIRS = {"logs", ".git", "__pycache__"}


# ============================================================
# UTILS: PATH / CREDS
# ============================================================

def win_to_wsl(path: str) -> str:
    """
    Convert Windows path (C:\\Users\\...) to WSL path (/mnt/c/Users/...).
    """
    drive, rest = path.split(":", 1)
    drive = drive.lower()
    rest = rest.replace("\\", "/")
    return f"/mnt/{drive}{rest}"


def load_credentials() -> dict:
    if not os.path.exists(CREDENTIALS_FILE):
        raise FileNotFoundError(f"Credentials file NOT found: {CREDENTIALS_FILE}")
    with open(CREDENTIALS_FILE, "r") as f:
        return json.load(f)


# ============================================================
# GITHUB PUSH (DEBUG MODE)
# ============================================================

def create_test_file(repo_path: str) -> Path:
    file_path = Path(repo_path) / "hello_from_vibecoder.txt"
    file_path.write_text(
        "Hello from DG_vibecoder! This is a debug test.\n",
        encoding="utf-8"
    )
    print(f"[OK] Test file created: {file_path}")
    return file_path


def set_git_remote(repo_path: str, username: str, token: str, repo_name: str):
    os.chdir(repo_path)
    remote_url = f"https://{username}:{token}@github.com/{username}/{repo_name}.git"
    print(f"[DEBUG] Setting git remote to: {remote_url}")

    if not Path(".git").exists():
        raise RuntimeError(f"ERROR: Not a git repository: {repo_path}")

    result = subprocess.run(
        ["git", "remote", "set-url", "origin", remote_url],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)
    if result.returncode != 0:
        raise RuntimeError("Failed to set git remote URL")

    print("[OK] Git remote updated.")


def git_push(repo_path: str, message: str):
    os.chdir(repo_path)

    print("[DEBUG] git add .")
    result = subprocess.run(["git", "add", "."], capture_output=True, text=True)
    print(result.stdout, result.stderr)

    print(f"[DEBUG] git commit -m \"{message}\"")
    result = subprocess.run(
        ["git", "commit", "-m", message],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    print("[DEBUG] git push origin main")
    result = subprocess.run(
        ["git", "push", "origin", "main"],
        capture_output=True, text=True
    )
    print(result.stdout, result.stderr)

    if result.returncode != 0:
        raise RuntimeError("Git push FAILED.")
    print("[OK] Git push successful.")


def run_debug_push():
    print("=================================================")
    print("=== DG_vibecoder â€” DEBUG PUSH MODE (v3.0) =======")
    print("=================================================")

    creds = load_credentials()
    username = creds["github"]["user"]
    token = creds["github"]["token"]
    repo_name = creds["deadlygraphics"]["repo_name"]
    win_repo_path = creds["deadlygraphics"]["paths"]["local_repo"]
    repo_path = win_to_wsl(win_repo_path)

    print(f"[DEBUG] Windows repo path: {win_repo_path}")
    print(f"[DEBUG] WSL repo path: {repo_path}")

    if not os.path.exists(repo_path):
        print(f"[FATAL] Repo path does NOT exist on WSL: {repo_path}")
        return

    print("[DEBUG] Repo path exists.")
    create_test_file(repo_path)
    set_git_remote(repo_path, username, token, repo_name)
    git_push(repo_path, "DG_vibecoder v3.0 debug push")

    print("=================================================")
    print("=== DEBUG PUSH MODE COMPLETE ====================")
    print("=================================================")


# ============================================================
# OVERSEER: DUMP (MARCO)
# ============================================================

def read_file(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8")
    except Exception as e:
        return f"<< ERROR READING FILE {path}: {e} >>"


def build_overseer_dump(target_folder: Path) -> str:
    """
    Build a snapshot string of all code/text files in target_folder.
    """
    lines = []
    lines.append("=== OVERSEER SNAPSHOT ===")
    lines.append(f"generated={datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"target_folder={target_folder}")
    lines.append("")

    lines.append("=== FILES ===")

    file_count = 0
    for root, dirs, files in os.walk(target_folder):
        # prune excluded dirs
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]

        for fname in files:
            ext = os.path.splitext(fname)[1].lower()
            if ext not in INCLUDE_EXT:
                continue

            abs_path = Path(root) / fname
            rel_path = abs_path.relative_to(target_folder)

            content = read_file(abs_path)
            file_count += 1

            lines.append(f"=== FILE START: {rel_path} ===")
            lines.append("<CONTENT>")
            lines.append(content)
            lines.append("</CONTENT>")
            lines.append("=== FILE END ===")
            lines.append("")

    lines.append("=== SUMMARY ===")
    lines.append(f"total_files={file_count}")
    lines.append("")
    return "\n".join(lines)


def run_overseer_dump():
    """
    MARCO:
      - Create per-timestamp snapshot (overseer_adjust.txt + history copy).
      - Overwrite logs/overseer_response.txt with the latest snapshot
        plus instructions for the LLM to add PATCH blocks.
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER DUMP MODE (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    print(f"[INFO] Snapshotting folder: {root_folder}")

    # Build snapshot text
    snapshot_text = build_overseer_dump(root_folder)

    # 1) Per-timestamp logs (history)
    logs_root = root_folder / "logs"
    logs_root.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = logs_root / timestamp
    history_dir = log_dir / "history"
    history_dir.mkdir(parents=True, exist_ok=True)

    adjust_path = log_dir / "overseer_adjust.txt"
    history_adjust_path = history_dir / "overseer_adjust.txt"

    adjust_path.write_text(snapshot_text, encoding="utf-8")
    history_adjust_path.write_text(snapshot_text, encoding="utf-8")

    print(f"[SUCCESS] Overseer adjust dump created at: {adjust_path}")
    print(f"[INFO] History copy saved at: {history_adjust_path}")

    # 2) Persistent working document at logs/overseer_response.txt
    working_doc_path = logs_root / "overseer_response.txt"

    response_template = f"""{snapshot_text}
=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.
"""

    working_doc_path.write_text(response_template, encoding="utf-8")
    print(f"[INFO] Working doc updated: {working_doc_path}")

    print("")
    print("Next steps:")
    print("  1) Open logs/overseer_response.txt and paste its content to ChatGPT.")
    print("  2) ChatGPT will insert PATCH blocks under the PATCHES section.")
    print("  3) Save the updated file, then run:")
    print("       python3 DG_vibecoder.py overseer-implement")
    print("=================================================")


# ============================================================
# OVERSEER: PATCH PARSING & APPLY (POLO)
# ============================================================

class Patch:
    def __init__(self, file_rel: str):
        self.file_rel = file_rel
        self.mode = None  # "replace" or "insert_after"
        self.find_text = ""
        self.replace_text = ""
        self.insert_after = ""
        self.insert_text = ""


def parse_patches(response_text: str):
    """
    Parse all PATCH START/END blocks from a large working document.
    Returns a list of Patch objects.
    """
    patches = []
    lines = response_text.splitlines()

    current = None
    state = None  # None, "FIND", "REPLACE", "INSERT_AFTER", "INSERT_TEXT"

    for raw in lines:
        line = raw.rstrip("\n")

        if line.startswith("=== PATCH START:"):
            # Extract raw filename
            file_rel_raw = line[len("=== PATCH START:"):].strip()

            # Remove trailing "===" or any '=' clutter
            if file_rel_raw.endswith("==="):
                file_rel_raw = file_rel_raw[:-3].rstrip()
            while file_rel_raw.endswith("="):
                file_rel_raw = file_rel_raw[:-1].rstrip()

            file_rel_raw = file_rel_raw.strip()

            current = Patch(file_rel_raw)
            patches.append(current)
            state = None
            continue

        if line.startswith("=== PATCH END"):
            current = None
            state = None
            continue

        if current is None:
            continue

        if line.strip() == "FIND:":
            current.mode = "replace"
            state = "FIND"
            current.find_text = ""
            continue

        if line.strip() == "REPLACE:":
            state = "REPLACE"
            current.replace_text = ""
            continue

        if line.strip() == "INSERT_AFTER:":
            current.mode = "insert_after"
            state = "INSERT_AFTER"
            current.insert_after = ""
            continue

        if line.strip() == "INSERT_TEXT:":
            state = "INSERT_TEXT"
            current.insert_text = ""
            continue

        # Accumulate text for the current state
        if state == "FIND":
            current.find_text += (line + "\n")
        elif state == "REPLACE":
            current.replace_text += (line + "\n")
        elif state == "INSERT_AFTER":
            current.insert_after += (line + "\n")
        elif state == "INSERT_TEXT":
            current.insert_text += (line + "\n")

    # Strip trailing newlines
    for p in patches:
        if p.find_text:
            p.find_text = p.find_text.rstrip("\n")
        if p.replace_text:
            p.replace_text = p.replace_text.rstrip("\n")
        if p.insert_after:
            p.insert_after = p.insert_after.rstrip("\n")
        if p.insert_text:
            p.insert_text = p.insert_text.rstrip("\n")

    return patches


def apply_patch_to_file(root_folder: Path, patch: Patch) -> str:
    """
    Apply a single patch to the appropriate file.
    Returns a human-readable summary line.
    """
    target_path = root_folder / patch.file_rel

    if not target_path.exists():
        return f"[WARN] File not found: {patch.file_rel}"

    original = target_path.read_text(encoding="utf-8")

    if patch.mode == "replace":
        if patch.find_text not in original:
            return f"[WARN] FIND text not found in {patch.file_rel}"
        new_content = original.replace(patch.find_text, patch.replace_text, 1)
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Replace applied to {patch.file_rel}"

    elif patch.mode == "insert_after":
        idx = original.find(patch.insert_after)
        if idx == -1:
            return f"[WARN] INSERT_AFTER text not found in {patch.file_rel}"

        insert_pos = idx + len(patch.insert_after)
        new_content = original[:insert_pos] + "\n" + patch.insert_text + original[insert_pos:]
        target_path.write_text(new_content, encoding="utf-8")
        return f"[OK] Insert-after applied to {patch.file_rel}"

    else:
        return f"[WARN] Unknown patch mode for {patch.file_rel}"


def run_overseer_implement():
    """
    POLO:
      - Read logs/overseer_response.txt
      - Parse all PATCH blocks
      - Apply patches to files
      - Write logs/last_patch_report.txt
    """
    print("=================================================")
    print("=== DG_vibecoder â€” OVERSEER IMPLEMENT (v3.0) ====")
    print("=================================================")

    root_folder = Path(__file__).resolve().parent
    logs_root = root_folder / "logs"
    working_doc_path = logs_root / "overseer_response.txt"

    if not working_doc_path.exists():
        print(f"[FATAL] Working doc not found: {working_doc_path}")
        return

    response_text = working_doc_path.read_text(encoding="utf-8")
    patches = parse_patches(response_text)

    if not patches:
        print("[WARN] No patches found in overseer_response.txt. Nothing to do.")
        return

    print(f"[INFO] Parsed {len(patches)} patch block(s). Applying...")

    report_lines = []
    report_lines.append(f"PATCH REPORT - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append(f"Working doc: {working_doc_path}")
    report_lines.append("")

    for p in patches:
        summary = apply_patch_to_file(root_folder, p)
        print(summary)
        report_lines.append(summary)

    report_text = "\n".join(report_lines)
    last_report_path = logs_root / "last_patch_report.txt"
    last_report_path.write_text(report_text, encoding="utf-8")

    print("")
    print(f"[INFO] Patch report written to: {last_report_path}")
    print("NOTE: Current implementation does NOT remove patch blocks from")
    print("      overseer_response.txt. You can manually clear or archive them")
    print("      once you're happy with the applied changes.")
    print("=================================================")


# ============================================================
# MAIN DISPATCHER
# ============================================================

def print_usage():
    print("DG_vibecoder v3.0 â€” modes:")
    print("")
    print("  python3 DG_vibecoder.py debug-push")
    print("      Run the original hello-file + git push test.")
    print("")
    print("  python3 DG_vibecoder.py overseer-dump")
    print("      MARCO: Overwrite logs/overseer_response.txt with a full snapshot")
    print("      of the DG_vibecoder folder plus instructions for the LLM.")
    print("")
    print("  python3 DG_vibecoder.py overseer-implement")
    print("      POLO: Read logs/overseer_response.txt, apply all PATCH blocks")
    print("      to the codebase, and write logs/last_patch_report.txt.")
    print("")


def main():
    if len(sys.argv) < 2:
        print_usage()
        return

    mode = sys.argv[1].lower()

    if mode == "debug-push":
        run_debug_push()
    elif mode == "overseer-dump":
        run_overseer_dump()
    elif mode == "overseer-implement":
        run_overseer_implement()
    else:
        print(f"[ERROR] Unknown mode: {mode}")
        print_usage()


if __name__ == "__main__":
    main()

</CONTENT>
=== FILE END ===

=== SUMMARY ===
total_files=4

=== PATCHES (TO BE FILLED BY LLM) ===

# Instructions:
# - ChatGPT should ADD patch blocks in this file using the format:
#
# === PATCH START: relative/path/to/file.py ===
# FIND:
# old text
# REPLACE:
# new text
# === PATCH END ===
#
# or:
#
# === PATCH START: relative/path/to/file.py ===
# INSERT_AFTER:
# line to match
# INSERT_TEXT:
# new text here
# === PATCH END ===
#
# No changes are applied until you run:
#   python3 DG_vibecoder.py overseer-implement
#
# You (the human) can paste the entire content of this file to ChatGPT,
# then paste ChatGPT's updated content back into this same file.

=== FILE END: modules/logs/overseer_response.txt ===

=== FILE START: update.py ===
#!/usr/bin/env python3
import subprocess
import sys

def run(cmd):
    print(f"\n[RUN] {' '.join(cmd)}")
    subprocess.run(cmd, text=True, check=True)

if __name__ == "__main__":
    print("=== Vibecoder Update Script (v3.0 Wired) ===")
    # Point to the SMART engine in modules
    engine = "modules/DG_vibecoder_github_push.py"
    
    # Run POLO (Implement) then MARCO (Dump)
    run(["python3", engine, "overseer-implement"])
    run(["python3", engine, "overseer-dump"])
    print("\n=== UPDATE COMPLETE ===")

=== FILE END: update.py ===

=== OVERSEER MANIFEST END ===
